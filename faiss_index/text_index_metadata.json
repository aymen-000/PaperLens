{
  "metadata": [
    {
      "chunk_type": "text",
      "chunk": 0,
      "content": "High-capacity associative memory in a quantum-optical spin glass Brendan P. Marsh,1, 2 David Atri Schuller,1, 2 Yunpeng Ji,1, 2, 3 Henry S. Hunt,2, 3 Surya Ganguli,1 Sarang Gopalakrishnan,4 Jonathan Keeling,5 and Benjamin L. Lev1, 2, 3 1Department of Applied Physics, Stanford University, Stanford, CA 94305, USA 2E. L. Ginzton Laboratory, Stanford University, Stanford, CA 94305, USA 3Department of Physics, Stanford University, Stanford, CA 94305, USA 4Department of Electrical and Computer Engineering, Princeton University, Princeton, NJ 08544, USA 5SUPA, School of Physics and Astronomy, University of St. Andrews, St. Andrews KY16 9SS, United Kingdom (Dated: September 16, 2025) The Hopfield model describes a neural network that stores memories using all-to-all-coupled spins. Memory patterns are recalled under equilibrium dynamics. Storing too many patterns breaks the associative recall process because frustration causes an exponential number of spurious patterns to arise as the network becomes a spin glass. Despite this, memory recall in a spin glass can be restored, and even enhanced, under quantum-optical nonequilibrium dynamics because spurious patterns can now serve as reliable memories. We experimentally observe associative memory with high storage capacity in a driven-dissipative spin glass made of atoms and photons. The capacity surpasses the Hopfield limit by up to seven-fold in a sixteen-spin network. Atomic motion boosts capacity by dynamically modifying connectivity akin to short-term synaptic plasticity in neural networks, realizing a precursor to learning in a quantum-optical system. Content-addressable associative memories can convert corrupted or incomplete data into clean, stored memory patterns through the process of pattern completion. For example, one may want to remember the face of a friend based on a blurry photo. Successful pattern completion outputs the unblurred image. (Too much blurring may result in the recall of a different friend.) Ordered states of matter can store such memories: For example, an Ising ferromagnet encodes one bit of information, namely whether the spins are mostly up or down. This encoding is highly redundant, and therefore robust against errors, but stores only one pattern as a memory, the spins-all- aligned state. Simultaneously maximizing both mem- ory storage capacity and robustness to recall error has wide-ranging implications from the fields of artificial in- telligence (AI), where dense associative memories have been closely linked to the transformer architecture that now dominates AI, particularly large language models [1], to neuroscience, where such associative systems are long thought to mediate human episodic memories [2]. The Hopfield model, a subject of the",
      "paper_id": "http://arxiv.org/abs/2509.12202v1"
    },
    {
      "chunk_type": "text",
      "chunk": 1,
      "content": "episodic memories [2]. The Hopfield model, a subject of the 2024 Nobel Prize in Physics, is an example of a physically motivated asso- ciative memory that can store multiple patterns [3–5]. It employs a recurrent neural network based on Ising spins that can i) learn and store new memories through a bio- logically plausible mechanism (Hebbian learning [6]), and ii) effect pattern completion via equilibrium dynamics such as Metropolis-Hastings (MH) dynamics [7]. These dynamics consist of a stochastic update that corresponds to equilibrium energy exchange with a heat bath. The network consists of n binary variables (called “spins” or “neurons”) si = ±1. The energy of a spin configura- tion is E = −P ij Jijsisj, where the Hebbian coupling weights (or “synapses”) are Jij = PP p=1 ξp i ξp j and each n-dimensional binary vector ξp i is a stored pattern, i.e., one of the intended stored memories. By analogy with the ferromagnet, one might expect that memories are retrievable in the Hopfield model so long as the system is in a (generalized) ferromagnetic phase, where the P patterns ξp i generalize the all up or down “pattern” in the conventional ferromagnet. In- deed, Amit et al. showed that such ferromagnetic re- trieval states exist for P less than the thermodynamic bound P0 ≈0.14n [10]. In this regime, the patterns ξp i form local minima of E, and pattern completion occurs via stochastic energy descent to those minima. How- ever, when P ≥P0, the retrieval states become ther- modynamically unstable and an equilibrium spin glass forms [11, 12]. This hampers recall through the formation of a rugged energy landscape with exponentially many lo- cal minima residing in deep, nested valleys separated by high energy barriers. Most minima do not coincide with any intended pattern ξp i and are thus “spurious.” More- over, the spurious patterns cannot serve as reliable mem- ories under MH dynamics: An error can lead to other patterns in nearby energy valleys. For this reason, spin glasses have long been considered incompatible with as- sociative memory. However, a recent theoretical work suggested that switching to a particular form of nonequilibrium dynam- ics would open access to the exponentially larger stor- age capacity of glassy systems [13]. Driven-dissipative dynamics in a system of atoms and photons were theo- retically found to induce energy-lowering spin flips at a rate proportional to the energy lost upon flipping. This",
      "paper_id": "http://arxiv.org/abs/2509.12202v1"
    },
    {
      "chunk_type": "text",
      "chunk": 2,
      "content": "a rate proportional to the energy lost upon flipping. This realizes deterministic, “steepest descent” dynamics that reduces the multiplicity of possible relaxation pathways, thereby enlarging the basin size of any given spurious pattern. (By contrast, under MH dynamics, any spin- flip that lowers the energy is equally likely to happen.) Decreased entropy generation due to steepest descent arXiv:2509.12202v1 [quant-ph] 15 Sep 2025 2 FIG. 1. Sketch of the apparatus and memory recall process for an n = 16 network. a) Transverse and longitudinal fields (light blue) pump the cavity. A digital micromirror device (DMD) scatters the longitudinal pump into 16 beams of phase 0 (π) colored blue (orange). Each imposes a longitudinal field fi onto a single atomic ensemble at position ri within the cavity midplane, thereby inputting into the network a stimulus field. (Tweezer traps not shown.) The intracavity field contains both local (blue and orange) and nonlocal (light blue) components. Atomic ensembles are represented as spheres. They are colored blue (orange) to denote their effective spin up (down) states, which scatter into local fields with 0 (π) phase. A camera holographically images the emitted cavity field [8, 9]. b) Example stimulus field pattern. Spin-flip errors compared to memory 1 are circled in red. Image taken by recording the transmission of the longitudinal pump beams through an empty cavity. Images are processed to show only local fields and normalized to the their maximum value. Remaining background is residual noise; see [9] for image analysis. Black bars are of length w0 = 34.8 µm. c) Memory recall dynamics in a low-dimensional representation of a spin-glassy energy landscape versus spin configuration. Red arrow indicates gradient descent from the corrupted stimulus pattern (red dot) in panel (b) to the memory pattern 1 at the bottom of its basin of attraction. Three additional representative memory patterns are indicated at their respective basin minima. d) Cavity output image showing the local fields from the atomic ensembles after successful recall of memory 1, starting from the stimulus field in panel (b). dynamics enhances recall fidelity, thereby turning what would have been spurious patterns under equilibrium re- call dynamics into high-fidelity memories under steepest descent dynamics. That is, the spurious pattern, resid- ing at the minimum of a basin of attraction, can now be considered a natural memory of the network defined by the spin-connectivity weights Jij. We experimentally demonstrate the enhanced memory capacity of a",
      "paper_id": "http://arxiv.org/abs/2509.12202v1"
    },
    {
      "chunk_type": "text",
      "chunk": 3,
      "content": "Jij. We experimentally demonstrate the enhanced memory capacity of a spin glass using an all-to-all Ising spin net- work created with multimode cavity QED [9]. Driven- dissipative dynamics are natural to cavity QED-based quantum-optical systems, and the theoretical possibil- ity for creating associative memory therein has been dis- cussed [13–16]. Not discussed, however, is a mechanism we report here by which the Jij evolve as dynamical quantities that depend on spin position and contributes to capacity enhancement. While many optics and non- linear optics experiments have realized associative mem- ory [17–20], none have observed memory enhancement due to spin glass ordering. “Dense” associative mem- ory networks can also possess enhanced capacity, though at the expense of engineering greater-than-2-body spin interactions [21, 22]. We also note that photonic neu- ral networks at the few-quanta level are under develop- ment [23]. We perform associative memory recall in artificial neu- ral networks of size up to n = 20 and extensively char- acterize networks with n ≤16. Bose-condensed gases of atoms play the role of the spins (neurons) while photons resonating in an optical cavity mediate the synaptic con- nections (Jij weights). The memory capacity can exceed the thermodynamic bound of the Hopfield model P0 by up to seven-fold at n = 16, using a 50% recall probability threshold; see supplementary materials [24] for a discus- sion of this threshold choice. Moreover, this is larger than the capacity limit equal to n attained by replacing Hebbian learning with a pseudoinverse learning rule [25]. The high memory capacity is attributed to two exper- imental features that are absent in the Hopfield model. First, our results are consistent with theoretical expecta- tions [13] (albeit derived in a different parameter regime) that a cavity-cooling mechanism [26] intrinsic to the ex- periment contributes to deterministic spin relaxation, rather than a solely stochastic energy descent. Second, the Jij depend on the spin positions ri that are not frozen in space, but can respond elastically to a “synaptic” stim- 3 ulus. This renders J a dynamical quantity through a spin-motion coupling Jij = J(ri, rj) and stabilizes mem- ory patterns. In response to optical forces generated by the spin-dependent cavity field, each atomic gas shifts its position to perturb its connectivity J(ri, rj) with all others. The modified Jij can then flip spins to drive the system deeper into a basin of attraction. The cavity field is modified with",
      "paper_id": "http://arxiv.org/abs/2509.12202v1"
    },
    {
      "chunk_type": "text",
      "chunk": 4,
      "content": "a basin of attraction. The cavity field is modified with each spin flip, inducing more movement, resulting in a self-reinforcing interplay between spin and motional degrees of freedom until the system evolves to a configuration deep within a basin of attraction. In con- sequence, the energy landscape elastically deforms under the driven-dissipative cavity dynamics to assist the spin evolution toward the memory. Simulations presented in Ref. [24] replicate this elastic response and indicate that this enhances memory capacity. Thus, we realize a form of “polaronic spin glass,” one whose connectivity dynam- ically changes to provide self-reinforced memory recall. This is akin to the creation of polarons in crystal lattices where the position of a charge leads to a deformation of the lattice, which in turn acts to trap the charge [27]. Our elastic enhancement of memory is analogous to “short-term synaptic plasticity” in neurobiology [28], where transient facilitation of synapses can enhance memory capacity [29]. This enhancement arises through transiently altered synaptic connectivity that deepens and reinforces the energy basin in which neural activ- ity currently resides. While the training of physical neu- ral networks has been considered [30], the utilization of synaptic self-reinforcement through natural dynamics in magnetic and photonic systems remains in early-stage development [31, 32]. The experimental system has been discussed in Refs. [9, 33]; see [24] for parameters and procedures spe- cific to this work. Figure 1a sketches the apparatus, which has been augmented to enable the application of bias fields onto each spin. Briefly, each spin is repre- sented by the motional excitation of a small rubidium Bose-Einstein condensate (BEC) trapped by an optical tweezer. These motional states are one of two possible checkerboard density waves representing collective spin up or down states. This yields an effective Ising spin degree of freedom ˆSx i [9]. The density wave states are created by the interference of a transverse pump laser and the field of the multimode cavity within which the BECs are confined [34–36]. BECs in spin up (down) density-wave states scatter the pump light into the cavity with phase 0 (π) with respect to this transverse pump. Recording the phase and amplitude of the cavity emission from each of these BECs using holographic imaging allows us to measure all ⟨ˆSx i ⟩of a microscopic spin configuration [8, 9, 37]; e.g., see Fig. 1d. Longitudinal pump beams provide local fields fi that bias these effective spins",
      "paper_id": "http://arxiv.org/abs/2509.12202v1"
    },
    {
      "chunk_type": "text",
      "chunk": 5,
      "content": "beams provide local fields fi that bias these effective spins toward a stimulus pattern ⟨ˆSx i ⟩∝fi; see Fig. 1b. (We note that rarely, a BEC splits into two opposite density waves of unequal weight. This does not affect our ability to determine the spin configuration; see Refs. [9, 24] for explanation.) FIG. 2. Memory recall fidelity. a) Recall probability versus number of spin-flip errors in memory pattern 1 of connectivity realization J1, shown as inset (same image as in Fig. 1d). Be- low are examples of three defective stimulus images including one, two, and four spin-flip errors, circled in red. The dashed line shows the basin size, which is chosen where the fitted curve (blue line) intersects the 50% probability threshold; a hyperbolic tangent is empirically found to be a suitable fitting function [24]. b-d) Recall curves for three additional memo- ries of J1. Insets are emission images showing the memory patterns. Black scale bars are of length w0. The multimode cavity light—comprised of thousands of near-degenerate modes [35, 37]—also induces all-to-all, sign-changing interactions among the effective spins [8, 14, 38]. When the BECs are spaced far enough from both the cavity center and each other, the interaction yields a frustrated Jij connectivity matrix sufficient to create a spin glass [13]. See Refs. [9, 24] for the form of this interaction and Refs. [9, 33] for measurements of the “overlap” order parameters [11] that characterize this in- trinsically nonequilibrium spin glass [33]—i.e., one that is coherently driven while under the influence of dissipation through cavity emission. We previously showed that the multimode cavity- mediated interactions yield a spin-glass Hamiltonian of Ising form when the cavity is tuned to the so-called “4/7” configuration of a Fabry-P´erot resonator [9]. The 4/7 cavity geometry is realized when the ratio of mir- ror spacing L to radius of curvature R is ∼1.22; here, R = 1 cm. The spin-glass Hamiltonian arises in the dispersive pumping limit, wherein the pumps are red- 4 detuned ∆C ≈−2π·20 MHz from the near-degenerate 4/7 cavity modes. The multimode Dicke model de- scription of the system then simplifies to a frustrated, transverse-field Ising model with longitudinal fields [24]: ˆH = ωz n X i=1 ˆSz i −g n X ij=1 J(ri, rj) ˆSx i ˆSx j − n X i=1 fi ˆSx i . (1) Each spin is represented by a collective spin operator ˆSx/y/z i of size",
      "paper_id": "http://arxiv.org/abs/2509.12202v1"
    },
    {
      "chunk_type": "text",
      "chunk": 6,
      "content": "represented by a collective spin operator ˆSx/y/z i of size S = M/2, where M ≈4·104 is the num- ber of atoms per ensemble, on average. The form of J(ri, rj) is in [24]. The experimental dynamics may be approximated by unitary evolution through Eq. (1) combined with dissipation using a Lindbladian treat- ment [13, 24]. The term proportional to the atomic recoil frequency ωz ≈2π·7.5 kHz plays the role of a transverse field. The strength of the interaction term is g = −g2 0Ω2/(∆2 A∆C), where the Rabi rate squared Ω2 is proportional to the transverse pump power, g0 = 2π·1.35 MHz is the Rb atom-cavity coupling strength, the detuning of the pumps from the atomic transition is ∆A ≈−2π·97.2 GHz, and the cavity linewidth is κ = 2π·140 kHz. The system is in the single-atom, large cooperativity limit of cavity QED, even without multi- mode field enhancement [24, 37]. Associative memory recall is performed as follows. The transverse pump is exponentially increased in power till the spin interactions are as large as 2π·2 kHz at 8 ms. Simultaneously, a memory stimulus is input into the net- work by ramping up the longitudinal fields over 3 ms. This induces |fi| whose strengths are approximately ωz. Figure 1b shows an example stimulus pattern for the con- nectivity matrix we call J1; see [24] for all Jij elements. The transverse and longitudinal pumps drive spin evolu- tion from the stimulus configuration down toward lower- energy configurations within a basin of attraction, as de- picted in Fig. 1c. The biasing fields are held constant for another ∼3 ms to allow spin organization to continue at fixed |fi|. We then ramp down fi before turning it off 1-ms before imaging to allow unbiased spin evolution toward a memory configuration; these are attractors of the dynamics. In this work, we do not explore long-term aging dynamics in the glassy landscape. An example of successful pattern recall is shown in Fig. 1d for what we designate as memory 1 of J1. Recall fidelity is qualita- tively insensitive to small changes in this procedure [24]. To operate the neural network again, fresh BECs are cre- ated and positioned to realize the same J, within exper- imental uncertainty [9, 24]. Different coupling matrices J realize different neural networks, each with their own memories. We study five disorder realizations of J for each system size n = 4,",
      "paper_id": "http://arxiv.org/abs/2509.12202v1"
    },
    {
      "chunk_type": "text",
      "chunk": 7,
      "content": "realizations of J for each system size n = 4, 8, 12 and 16. To obtain different J’s at fixed n, we simply move the ensembles within the cavity midplane. This causes the Jij to change sign in a sufficiently random manner that the resulting glassy energy landscapes store distinct sets of memory patterns [9, 13, 33]. In practice, it suf- fices to trap the ensembles in a rectilinear array of spacing roughly 50 µm from one another, and few-micron-scale adjustments to the row and column locations yield sig- nificantly different J’s [24]. We report memories recalled from a single J at n = 20 in [24]. Note that the rapid proliferation of memories renders capacity measurements prohibitively time consuming for n > 16, at present. Memory capacities are determined by cataloging all memories with finite basins of attraction that are natu- rally stored by each J. That is, the J are not trained to realize specific patterns in this work. Rather, we sample the patterns found by performing up to 400 experimental recall cycles, each with the same J but a randomly se- lected stimulus pattern {fi}. A hierarchical clustering al- gorithm [39] organizes the observed patterns into groups of similar patterns to allow for small fluctuations in ⟨ˆSx i ⟩ between experimental shots [24]. Each group serves as a “candidate memory.” We then use the most commonly found pattern in each group as the reference pattern to measure the basin size [24]. We define the basin size of a candidate memory as the average number of randomized spin flip errors that may be tolerated in the stimulus while exceeding a 50% recall probability threshold. We find an average of 51 candidates for the five J’s studied at n = 16. However, most candidates exhibit basin sizes smaller than one spin flip and are therefore not useful as associative memories. This is expected: Reference [13] predicted a typical basin size of the spin glass to be ∼0.013n, which is less than one for n = 16. Thus, for small n, it is reasonable to restrict “memories” to mean only those memory candi- dates with basin sizes greater than or equal to one spin flip. The capacity we quote for each J is the number of memories that satisfy this criterion. We estimate that we find >95% of all such memories for n ≤16 [24]. Figure 2 shows recall fidelity",
      "paper_id": "http://arxiv.org/abs/2509.12202v1"
    },
    {
      "chunk_type": "text",
      "chunk": 8,
      "content": "memories for n ≤16 [24]. Figure 2 shows recall fidelity curves for representative memories of J1. Memory 1 in panel (a) exhibits a par- ticularly large basin size of nearly four spin flips. Fig- ures 2b-d present typical recall curves for three additional memories of J1, with basin sizes between 1-3 spin flips. We find J1 has a memory capacity of 14(1) memories [24]. See [24] for a gallery of the ten other memories. We measure the capacity for all J and n and plot these in Fig. 3a. The average capacity 11.9(6) of the n = 16 network is much greater than the 3.6 average number of memories storable by the Hopfield model [24]. As expected, however, the average basin size of the mem- ories of J1 is 2.1, which is lower than that of the Hop- field model, 3.9. This highlights the trade-off noted in Ref. [13] between a spin glass’s larger capacity and its smaller average basin sizes; ultimately, specific applica- tion requirements may prefer one over the other. Plotting the number of natural memories in the Sherrington-Kirkpatrick (SK) model [11, 40] allows us to compare to a model that is known to possess an expo- nential number of minima in n [41]. (The SK model de- scribes an all-to-all spin glass similar to what we realize; see [9, 13, 33] for a discussion.) We find that the mean experimental capacity is consistent with the SK model 5 FIG. 3. Memory capacity and average basin volume versus n. a) Memory capacity of five different J’s for each of four different system sizes n. Their average is shown as red diamonds. Error bars are standard error and stem from basin size estimation [24]. Here and in panel (b), arrows indicate the polaronic enhancement of memory capacity for the J1 network, where capacity is measured at both the nominal trap elasticity condition (blue circles) and at a higher elasticity (gray triangle). The solid curves show the simulated memory capacity in the SK spin glass under steepest descent (SD, blue) or Metropolis-Hastings (MH, orange) dynamics. The dashed black curve is for the Hopfield model. The SK (Hopfield) simulations include 1,000 (10,000) independent disorder realizations of J per n. The light blue (orange) band shows the standard deviation in SD (MH) capacities due to random variations in the disordered energy landscape versus J. This band is ∼1.3 memories for the Hopfield",
      "paper_id": "http://arxiv.org/abs/2509.12202v1"
    },
    {
      "chunk_type": "text",
      "chunk": 9,
      "content": "versus J. This band is ∼1.3 memories for the Hopfield model [24]. b) Comparison of the measured average basin volumes to the upper bounds (dashed lines) at each n. Capacity limits restrict Hopfield networks to lie within the shaded region. under steepest descent (SD) dynamics (up to n = 16). Moreover, their spreads in capacity versus disorder re- alization J are consistent. By contrast, the SK model simulated under MH dynamics does not match. While we cannot yet directly prove that purely steepest descent dynamics are at play, recording the spins’ temporal evo- lution would provide direct evidence. We further note that the match with SD dynamics may provide only a lower bound to the possible enhancement to memory ca- pacity for two reasons: 1) As we discuss below, ensemble position elasticity can double the memory capacity; and 2) simulations in section IV.C [24] show that eliminating the DMD phase noise on the stimulus field can improve capacity by 2-3×, pointing to a straightforward technical path toward further capacity enhancement. We also note that one can quantify the recall perfor- mance through the basin volume, defined as the number of spin configurations that flow to a given memory pat- tern. An ideal associative memory utilizes the full spin- configuration space, such that all stimulus patterns flow to memories possessing high recall fidelity. This corre- sponds to an upper bound on the average basin volume per memory of 2n divided by the memory capacity. We find that experimentally derived basin volumes, shown in Fig. 3b, come close to saturating this bound. Experimental observations additionally reveal a new aspect of the dynamics not considered in Ref. [13]: We notice that the atomic ensembles shift in position depend- ing on which memory pattern is recalled. This aforemen- tioned polaronic effect occurs because the tweezer poten- tials are not infinitely stiff, and thus optical forces from the emergent cavity field shift the atomic ensembles to positions away from their trap minima. Figure 4a shows a typical example of this effect: After memory recall, several ensembles are observed at locations up to 4 µm away from the trap minima, which is 20% of the tweezer waist. Moreover, the data indicate a spin-configuration dependence to the movement, because certain ensembles move differently when recalling one memory pattern ver- sus another. Reducing the transverse pump power down toward zero reverts the ensembles back to the tweezer trap",
      "paper_id": "http://arxiv.org/abs/2509.12202v1"
    },
    {
      "chunk_type": "text",
      "chunk": 10,
      "content": "toward zero reverts the ensembles back to the tweezer trap centers, rendering this phenomenon a form of elas- tic rather than plastic response. This elastic response acts to change the positions, and therefore the connec- tivity. Simulations in section IV.C [24] indicate that the motional shift deepens the energy well of the spin config- urations through which the system evolves, which then enhances recall probability of the nearby memory. This can lead to the generation of new memory patterns that are self-consistently stable under both spin and positional dynamics, increasing memory capacity. We experimentally verify that the J1 memory capac- ity increases when the polaronic deformations are made stronger. The laser power of each tweezer is lowered by a factor of four to yield a weaker, and therefore more elastic, trap. Figure 3a shows that this yields an almost two-fold increase in memory capacity for J1, resulting in 25(2) memories in a n = 16 network, 7-times the Hopfield capacity. We also measure an increased elastic response: The positions shown in Fig. 4b are on average ∼38% fur- ther away from the initial tweezer locations. However, it does not saturate the basin volume bound quite as well. In addition to enhancing capacity, simulations indicate that polaronic elasticity inhibits the effects of “J-chaos” on recall and memory capacity; J-chaos refers to large changes in local minima induced by small changes to the 6 FIG. 4. Polaronic response of atomic ensemble positions. a) Each dot is the center-of-mass (CoM) position of an ensemble within the 4×4 array of the n = 16 network. Grid lines at xi and yj indicate the tweezer trap locations. The scale bar is 5 µm. Positions of the ensembles at the time of imaging are shown in blue (red) after successful recall of memory pattern 1 (2) of J1. These are determined by fits to recall images like that shown in Fig. 1d. The CoM data from a total of 24 experimental shots are displayed per memory pattern. The average deviation of the CoM data from the trap locations is 1.32(2) µm. b) Same as panel (a), but with trap power re- duced four-fold to enhance polaronic elasticity, corresponding to the triangle data point in Fig. 3. The average deviation increases to 1.82(3) µm. Note that the spacings between ad- jacent xi (and yj) are decreased by ∼60% to visually magnify the spread in positions. Jij [42,",
      "paper_id": "http://arxiv.org/abs/2509.12202v1"
    },
    {
      "chunk_type": "text",
      "chunk": 11,
      "content": "∼60% to visually magnify the spread in positions. Jij [42, 43]. In simulations where elasticity is fully re- moved, the 0.5-µm-scale experimental drift of tweezer positions reduces the J1 memory capacity by 36%; see section IV.C [24]. In contrast, simulations with the level of elasticity present in the experiments of Fig. 4b show no reduction of memory capacity due to position noise. Future work could engineer a long-term form of plas- ticity that persists even without the pump field. This could be achieved, e.g., by shaping optical potentials to have additional minima in learned locations. Plastic self- reinforcement of memories in this quantum-optical sys- tem would realize a natural form of learning while enjoy- ing capacity enhancement and robustness to coupling er- rors. Photonic network denoisers and amplifiers are two possible applications once intracavity networks are in- creased in size using larger tweezer arrays [44]. Capacity- enhanced quantum associative memory might be possible using intracavity ensembles acting as effective spin-1/2 degrees of freedom [45]. In summary, this work experimentally reveals a new paradigm of associative memory, with enhanced memory capacity that goes far beyond the Hopfield limit, by ex- ploiting naturally occurring energy minima in a glassy energy landscape that the original Hopfield model must avoid. Such glassy minima can nevertheless yield robust memories in our system, because they are stabilized not only by steepest rather than stochastic energy descent, but also by a form of polaronic elasticity induced by self- reinforcing spin-motion coupling. This yields a quantum- optical realization of the short-term synaptic plasticity found in neurobiology. We thank Ronen Kroeze, Giulia Socolof, and De- ven Bowman for helpful discussions and experimental assistance. We are grateful for funding support from the Army Research Office (Grant #W911NF2210261). Y.J. acknowledges support from the Q-NEXT DOE National Quantum Information Science Research Cen- ter. J.K. acknowledges support from EPSRC (Grant No. EP/Z533713/1). B.M. acknowledges funding from the Stanford QFARM Initiative. H.H. acknowledges sup- port from the Stanford Shoucheng Zhang Graduate Fel- lowship. S.G. thanks the Schmidt Science Polymath pro- gram for support. [1] H. Ramsauer, B. Sch¨afl, J. Lehner, P. Seidl, M. Widrich, T. Adler, L. Gruber, M. Holzleitner, M. Pavlovi´c, G. K. Sandve, V. Greiff, D. Kreil, M. Kopp, G. Klambauer, J. Brandstetter, and S. Hochreiter, “Hopfield Networks is All You Need,” (2020), arXiv:2008.02217. [2] D. Marr, “Simple memory: a theory for archicortex,” Phil. Trans. R. Soc. Lond. B 262, 23 (1971).",
      "paper_id": "http://arxiv.org/abs/2509.12202v1"
    },
    {
      "chunk_type": "text",
      "chunk": 12,
      "content": "archicortex,” Phil. Trans. R. Soc. Lond. B 262, 23 (1971). [3] J. J. Hopfield, “Neural networks and physical systems with emergent collective computational abilities.” Proc. Natl. Acad. Sci. U.S.A. 79, 2554 (1982). [4] J. Hopfield and D. Tank, “Computing with neural cir- cuits: a model,” Science 233, 625 (1986). [5] W. Bialek, “Moving boundaries: An appreciation of John Hopfield,” (2024), arXiv:2412.18030. [6] D. O. Hebb, The Organization of Behavior (Wiley & Sons, New York, 1949). [7] N. Metropolis, A. W. Rosenbluth, M. N. Rosenbluth, A. H. Teller, and E. Teller, “Equation of state calcu- lations by fast computing machines,” J. Chem. Phys. 21, 1087 (1953). [8] Y. Guo, R. M. Kroeze, V. D. Vaidya, J. Keeling, and B. L. Lev, “Sign-Changing Photon-Mediated Atom In- teractions in Multimode Cavity Quantum Electrodynam- ics,” Phys. Rev. Lett. 122, 193601 (2019). [9] B. P. Marsh, D. A. Schuller, Y. Ji, H. S. Hunt, G. Z. Socolof, D. P. Bowman, J. Keeling, and B. L. Lev, “A multimode cavity QED Ising spin glass,” (2025), arXiv:2505.22658. [10] D. J. Amit, H. Gutfreund, and H. Sompolinsky, “Storing Infinite Numbers of Patterns in a Spin-Glass Model of Neural Networks,” Phys. Rev. Lett. 55, 1530 (1985). [11] D. L. Stein and C. M. Newman, Spin Glasses and Com- plexity, Primers in Complex Systems (Princeton Univer- sity Press, 2013). [12] P. Charbonneau, E. Marinari, M. M´ezard, G. Parisi, F. Ricci-Tersenghi, G. Sicuro, and F. Zamponi, Spin Glass Theory and Far Beyond (World Scientific, 2023). [13] B. P. Marsh, Y. Guo, R. M. Kroeze, S. Gopalakrish- nan, S. Ganguli, J. Keeling, and B. L. Lev, “Enhancing Associative Memory Recall and Storage Capacity Using Confocal Cavity QED,” Phys. Rev. X 11, 021048 (2021). [14] S. Gopalakrishnan, B. L. Lev, and P. M. Goldbart, 7 “Frustration and Glassiness in Spin Models with Cavity- Mediated Interactions,” Phys. Rev. Lett. 107, 277201 (2011). [15] S. Gopalakrishnan, B. L. Lev, and P. M. Goldbart, “Ex- ploring models of associative memory via cavity quantum electrodynamics,” Philos. Mag. 92, 353 (2012). [16] P. Rotondo, M. Marcuzzi, J. P. Garrahan, I. Lesanovsky, and M. M¨uller, “Open quantum generalisation of Hop- field neural networks,” J. Phys. A: Math. Theor. 51, 115301 (2018). [17] R. Xu, P. Lv, F. Xu, and Y. Shi, “A survey of ap- proaches for implementing optical neural networks,” Op- tics & Laser Technology 136, 106787 (2021). [18] B. J. Shastri, A. N. Tait, T. Ferreira de",
      "paper_id": "http://arxiv.org/abs/2509.12202v1"
    },
    {
      "chunk_type": "text",
      "chunk": 13,
      "content": "[18] B. J. Shastri, A. N. Tait, T. Ferreira de Lima, W. H. P. Pernice, H. Bhaskaran, C. D. Wright, and P. R. Pruc- nal, “Photonics for artificial intelligence and neuromor- phic computing,” Nat. Photonics 15, 102 (2021). [19] M. Katidis, K. Musa, S. Kumar, Z. Li, F. Long, C. Qu, and Y.-P. Huang, “Robust pattern retrieval in an optical Hopfield neural network,” Opt. Lett. 50, 225 (2025). [20] K. P. Kalinin et al., “Analog optical computer for AI inference and combinatorial optimization,” Nature 645, 354 (2025). [21] D. Krotov and J. J. Hopfield, in Advances in Neural In- formation Processing Systems, Vol. 29, edited by D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett (Curran Associates, Inc., 2016). [22] K. Musa, S. Kumar, M. Katidis, and Y.-P. Huang, “Dense Associative Memory in a Nonlinear Optical Hop- field Neural Network,” (2025), arXiv:2506.07849. [23] S.-Y. Ma, T. Wang, J. Laydevant, L. G. Wright, and P. L. McMahon, “Quantum-limited stochastic optical neural networks operating at a few quanta per activa- tion,” Nat Commun 16, 359 (2025). [24] See supplementary materials for experimental details and numerical simulations. [25] A. Storkey and R. Valabregue, “The basins of attraction of a new Hopfield learning rule,” Neural Networks 12, 869 (1999). [26] V. Vuleti´c and S. Chu, “Laser Cooling of Atoms, Ions, or Molecules by Coherent Scattering,” Phys. Rev. Lett. 84, 3787 (2000). [27] C. Franchini, M. Reticcioli, M. Setvin, and U. Diebold, “Polarons in materials,” Nat Rev Mater 6, 560 (2021). [28] R. S. Zucker and W. G. Regehr, “Short-Term Synaptic Plasticity,” Annu. Rev. Physiol. 64, 355 (2002). [29] J. F. Mejias and J. J. Torres, “Maximum Memory Ca- pacity on Neural Networks with Short-Term Synaptic Depression and Facilitation,” Neural Comput. 21, 851 (2009). [30] A. Momeni et al., “Training of physical neural networks,” Nature 645, 53 (2025). [31] J. Feldmann, N. Youngblood, C. D. Wright, H. Bhaskaran, and W. H. P. Pernice, “All-optical spiking neurosynaptic networks with self-learning capabilities,” Nature 569, 208 (2019). [32] C. Niu, H. Zhang, C. Xu, W. Hu, Y. Wu, Y. Wu, Y. Wang, T. Wu, Y. Zhu, Y. Zhu, W. Wang, Y. Wu, L. Yin, J. Xiao, W. Yu, H. Guo, and J. Shen, “A self- learning magnetic Hopfield neural network with intrinsic gradient descent adaption,” Proc. Natl. Acad. Sci. U.S.A. 121, e2416294121 (2024). [33] R. M. Kroeze, B. P. Marsh, D. Atri Schuller, H. S. Hunt,",
      "paper_id": "http://arxiv.org/abs/2509.12202v1"
    },
    {
      "chunk_type": "text",
      "chunk": 14,
      "content": "Kroeze, B. P. Marsh, D. Atri Schuller, H. S. Hunt, A. N. Bourzutschky, M. Winer, S. Gopalakrish- nan, J. Keeling, and B. L. Lev, “Directly observing replica symmetry breaking in a vector quantum-optical spin glass,” Science 389, 1122 (2025). [34] K. Baumann, C. Guerlin, F. Brennecke, and T. Esslinger, “Dicke quantum phase transition with a superfluid gas in an optical cavity,” Nature 464, 1301 (2010). [35] A. J. Koll´ar, A. T. Papageorge, K. Baumann, M. A. Ar- men, and B. L. Lev, “An adjustable-length cavity and Bose–Einstein condensate apparatus for multimode cav- ity QED,” New J. Phys. 17, 043012 (2015). [36] F. Mivehvar, F. Piazza, T. Donner, and H. Ritsch, “Cav- ity QED with quantum gases: new paradigms in many- body physics,” Adv. Phys. 70, 1 (2021). [37] R. M. Kroeze, B. P. Marsh, K.-Y. Lin, J. Keeling, and B. L. Lev, “High Cooperativity Using a Confocal- Cavity–QED Microscope,” PRX Quantum 4, 020326 (2023). [38] Y. Guo, V. D. Vaidya, R. M. Kroeze, R. A. Lunney, B. L. Lev, and J. Keeling, “Emergent and broken symmetries of atomic self-organization arising from Gouy phase shifts in multimode cavity QED,” Phys. Rev. A 99, 053818 (2019). [39] Z. Bar-Joseph, D. K. Gifford, and T. S. Jaakkola, “Fast optimal leaf ordering for hierarchical clustering,” Bioin- formatics 17, S22 (2001). [40] D. Sherrington and S. Kirkpatrick, “Solvable Model of a Spin-Glass,” Phys. Rev. Lett. 35, 1792 (1975). [41] K. Binder and A. P. Young, “Spin glasses: Experimen- tal facts, theoretical concepts, and open questions,” Rev. Mod. Phys. 58, 801 (1986). [42] A. J. Bray and M. A. Moore, “Chaotic Nature of the Spin-Glass Phase,” Phys. Rev. Lett. 58, 57 (1987). [43] Z. Zhu, A. J. Ochoa, S. Schnabel, F. Hamze, and H. G. Katzgraber, “Best-case performance of quantum anneal- ers on native spin-glass benchmarks: How chaos can affect success probabilities,” Phys. Rev. A 93, 012317 (2016). [44] H. J. Manetsch, G. Nomura, E. Bataille, K. H. Leung, X. Lv, and M. Endres, “A tweezer array with 6100 highly coherent atomic qubits,” (2024), arXiv:2403.12021. [45] B. P. Marsh, R. M. Kroeze, S. Ganguli, S. Gopalakr- ishnan, J. Keeling, and B. L. Lev, “Entanglement and Replica Symmetry Breaking in a Driven-Dissipative Quantum Spin Glass,” Phys. Rev. X 14, 011026 (2024). [46] K. Hepp and E. H. Lieb, “On the superradiant phase transition for molecules in a quantized radiation field: the dicke maser model,” Ann. Phys. 76,",
      "paper_id": "http://arxiv.org/abs/2509.12202v1"
    },
    {
      "chunk_type": "text",
      "chunk": 15,
      "content": "quantized radiation field: the dicke maser model,” Ann. Phys. 76, 360 (1973). [47] P. Kirton, M. M. Roses, J. Keeling, and E. G. Dalla Torre, “Introduction to the Dicke Model: From Equilibrium to Nonequilibrium, and Vice Versa,” Adv. Quantum Technol. 2, 1800043 (2018). [48] A. T. Papageorge, A. J. Koll´ar, and B. L. Lev, “Coupling to modes of a near-confocal optical resonator using a dig- ital light modulator,” Opt. Express 24, 11447 (2016). [49] Y. Guo, R. M. Kroeze, B. P. Marsh, S. Gopalakrish- nan, J. Keeling, and B. L. Lev, “An optical lattice with sound,” Nature 599, 211 (2021). [50] R. M. Kroeze, Y. Guo, V. D. Vaidya, J. Keeling, and B. L. Lev, “Spinor Self-Ordering of a Quantum Gas in a Cavity,” Phys. Rev. Lett. 121, 163601 (2018). [51] C. Pethick and H. Smith, Bose-Einstein condensation in dilute gases (Cambridge University Press, 2002). 8 SUPPLEMENTARY INFORMATION CONTENTS References 6 I. Experimental methods 8 II. Associative memory methods 10 III. Theoretical description 12 A. Quantum-optical model 13 B. Effective spin model 13 C. Semiclassical model 15 IV. Numerical simulations 16 A. Hopfield model memory capacity 16 B. SK model spin glass memory capacity 17 C. Simulation of quantum-optical neural networks 18 V. Extended data 21 A. J1 memory candidates 21 B. Memory capacity using 75% recall threshold 22 C. A 20-site neural network 23 I. EXPERIMENTAL METHODS Networks of ultracold gases of 87Rb are prepared in a multimode optical cavity as described in Ref. [9], with adjustments as follows. An average of M = 4.2(3) × 104 atoms are trapped at each site of the network and have been evaporatively cooled below the critical temperature for Bose-Einstein condensation (BEC); we measure an average BEC fraction of 17% with a 4% variation across sites. While Bose-condensation aids in preparing low-entropy initial states of the neural network, it is not a requirement for this work. The atom numbers are controlled to balance the signal strength per site during readout, as described below. This results in an approximately 25% variation in atom number between sites that does not change between experimental cycles. This variation does not degrade recall performance. Rather, it serves to roughly equalize the strength of the cavity-mediated interactions between ensembles by balancing the number of photons emitted by each spin ensemble. Optical tweezers are formed by crossed dipole traps, each with approximately a 20-µm waist and trap frequencies of",
      "paper_id": "http://arxiv.org/abs/2509.12202v1"
    },
    {
      "chunk_type": "text",
      "chunk": 16,
      "content": "each with approximately a 20-µm waist and trap frequencies of [ωx, ωy, ωz] = 2π·[326(5), 472(16), 332(9)] Hz. This yields an atomic density distribution ρ(r) with a 1/e radius of approximately 7 µm at each site. The tweezers form a rectilinear grid of nr rows and nc columns. The BECs are trapped at the n = nr · nc vertices of the grid to create networks of size n in the midplane of the cavity. Different neural networks are realized by selecting random locations for the rows and columns of the grid, following the method described in Ref. [9]. The minimum spacing between rows and columns is 40 µm, and the total spatial extent of the grid never exceeds 150 µm. The center of the grid deviates from the center of the cavity by up to 10 µm. The column locations for J1 shown in Fig. 4 of the main text are [x1, x2, x3, x4] = [−79, −23, 34, 89] µm and the row locations are [y1, y2, y3, y4] = [−94, −31, 28, 91] µm, both with respect to cavity center. A “4/7” multimode optical cavity mediates atomic interactions to realize the Jij connections of the neural network. The cavity is formed by a pair of R = 1 cm radius of curvature mirrors separated by a distance L ≈1.22 cm, with a free spectral range (FSR) of 2π·12.30010(6) GHz. The field decay rate reported in the main text is nearly constant for the modes participating in the 4/7 resonance [37]. The cavity geometry approximately satisfies the M/N multimode degeneracy condition L/R = 2 sin2(Mπ/2N) for the irreducible fraction M/N = 4/7 [9, 38]. This means that the cavity hosts N families of near-degenerate modes per FSR. Each family is identified by an integer 0 ≤η < N and contains the set of all Hermite-Gaussian modes Ξlm with constant l + m ≡η (mod N), up to a high-order mode cutoff [37]. We choose the family with η = 0 in this work for simplicity; the remaining mode families are far-detuned from the transverse pump frequency and do not contribute. The number M controls the longitudinal character of the mode families; cavities with odd M support standing-wave mode superpositions of arbitrary phase in the cavity midplane, while those superpositions at the midplane of even-M cavities are restricted to being either 0 or π in phase. By employing an",
      "paper_id": "http://arxiv.org/abs/2509.12202v1"
    },
    {
      "chunk_type": "text",
      "chunk": 17,
      "content": "being either 0 or π in phase. By employing an even-M cavity in this work, we realize cavity-mediated interactions of Ising form [9], rather than the 9 FIG. S1. a) Example stimulus field imaged through the cavity in the absence of atoms. The image is normalized to its maximum amplitude and processed to remove nonlocal components of the cavity field [9]. Each bright spot indicates a longitudinal field fi that couples to spin ˆSx i . The black bar indicates the waist size w0 = 34.8 µm for the fundamental mode of the cavity. b) Transverse (blue) and longitudinal (red) pump schedule for recall experiments. The dashed line indicates where imaging begins for spin readout. vector form realized in odd-M cavities like confocal cavities [33]. The explicit form of the cavity-mediated interaction Jij is provided in Eq. (S8). The atomic ensembles scatter photons from the λ = 780-nm transverse pump into the cavity modes, realizing a multimode variant of the Hepp-Lieb-Dicke model [36, 46, 47]. The system lies in the large cooperativity limit of cavity QED with a single-atom, single-mode coupling strength g0 = 2π·1.35 MHz, excited-state decay rate Γ = 2π·6.065 MHz, and single-mode cooperativity C = 4.28 [9]. Multimode enhancement in the dispersive coupling limit [37] yields an estimated multimode cooperativity of Cmm = 29. The pump frequency is detuned by ∆A ≈−2π·97 GHz from the D2 transition of 87Rb and by ∆C = −2π·20 MHz from the η = 0 cavity resonance frequency. Superradiant scattering into the cavity occurs above a critical pump strength Ωc given by MΩ2 cg2 0/∆2 A = −2Er(∆2 C + κ2)/(∆Cλmax), where Er = 3.7 kHz is the atomic recoil energy, λmax > 0 is the largest eigenvalue of the J matrix [45], and small dispersive shifts have been disregarded. Threshold-less scattering occurs in the presence of the longitudinal pump fields. Self-organization of each BEC into one of two possible “checkerboard” density wave states occurs concomitantly with superradiant scattering. The density-wave states map onto an SU(2) collective spin degree of freedom ˆSx/y/z i for each BEC in the network [9]. Holographic imaging of light emitted from the cavity [8] provides individual spin-state readout of the ⟨ˆSx i ⟩spin components as well as the center-of-mass (CoM) positions ri for each BEC after processing of the cavity field, as shown in Fig. 1d and described in Ref. [9]. Inhomogeneity in the magnitude of the",
      "paper_id": "http://arxiv.org/abs/2509.12202v1"
    },
    {
      "chunk_type": "text",
      "chunk": 18,
      "content": "described in Ref. [9]. Inhomogeneity in the magnitude of the measured spins |⟨ˆSx i ⟩| naturally arises due to spatial variations in the multimode cavity coupling, as described in Sec. III. The atom numbers per site are tuned to minimize this effect, yielding measured ⟨ˆSx i ⟩that vary in magnitude by less than 10% across sites, on average. Only the sign of ⟨ˆSx i ⟩is considered when judging whether a memory has been successfully recalled. Longitudinal fields fi can stimulate arbitrary spin configurations and enable associative memory recall. To generate the fields, a portion of the transverse pump laser is split off, shaped by a digital micromirror device (DMD), and injected directly (longitudinally) into one of the cavity mirrors. The field is shaped in the Fourier plane by the DMD [48, 49] to yield an array of n beams propagating in parallel to the cavity axis. Each individually targets an atomic ensemble at its CoM location ri, as illustrated in Fig. S1a. Each beam is focused to within 1 µm of ri with a waist of 7.3 µm. Their intensities and phases are fully tunable with respect to the transverse pump. The relative intensities are matched between sites to within 10%. The longitudinal field strengths |fi| are proportional to the geometric mean of the longitudinal beam intensity and the transverse pump intensity, as described in Sec. III. The relative phases between the longitudinal beams and the transverse pump encode the spin state that stimulates the atoms: A spin-up (spin- down) stimulus corresponds to 0 (π) phase with fi > 0 (fi < 0). The phases are controlled to within <0.3 radians between different beams. However, the global phase between the longitudinal beams and the transverse pump is uncontrolled and exhibits a slow drift between experimental cycles. As shown in Sec. III, this introduces an amplitude factor fi →cos(ϕ)fi for the random (i-independent) angle offset ϕ arising in each experimental sequence. This does not affect the spin configuration being stimulated, but does affect the strength of the applied stimulus. Weak stimuli can inhibit associative memory recall, though we are able to mitigate this problem by increasing the strength of the longitudinal beams. The simulations of Sec. IV include all these noise effects in their estimates of memory capacity. The ramp schedules for the longitudinal fields and transverse pump are optimized for associative memory recall fidelity and are presented in Fig. S1b.",
      "paper_id": "http://arxiv.org/abs/2509.12202v1"
    },
    {
      "chunk_type": "text",
      "chunk": 19,
      "content": "associative memory recall fidelity and are presented in Fig. S1b. The transverse pump is ramped exponentially to g = 4gc, where gc = g0Ωc/∆A is the threshold coupling strength for the superradiant transition. This drives spin-flip dynamics away from the stimulated state and towards memory states. The pump is quenched at 7.7 ms to ∼4.5gc for 300 µs to maximize photon flux for spin readout. No longitudinal fields are present during imaging. We find that the recall fidelity and 10 FIG. S2. Bootstrap estimation of the experimental memory capacity. a) Bootstrap samples are generated by resampling with replacement from the set of spin configurations found via random sampling. The number of memories present in each bootstrap sample is counted and averaged over 500 independent bootstrap samples for each sample size on the x axis. The resulting curves of memory capacity versus sample size are averaged over J disorder realizations for each n. The asymptote of each curve, corresponding to the true, average memory capacity at each n, is estimated via a scaling analysis shown in panel (b). This analysis estimates that for n = [16, 12, 8, 4] we find [95%, 96%, 96%, 99%] of the total number of memories. b) The same data as panel (a) are plotted versus the inverse of the number of samples. The y-intercepts of the curves thus correspond to the asymptotic memory capacities. These are estimated by linear extrapolation of the data to x = 0. basin size of memories are insensitive to small changes in this ramp schedule, such as making the entire sequence up to 20% longer or shorter, or adjusting the precise time at which the longitudinal fields are turned off. Each experimental trial yields a single output of the neural network, i.e., the spin states ⟨ˆSx i ⟩. The ⟨ˆSx i ⟩are extracted through holographic imaging [8, 50] of the cavity output over a 300 µs period as described in Ref. [9] and summarized as follows: Each image is demodulated to produce a phase-sensitive image of the electric field in the cavity midplane before downsampling ×4 in both directions and applying a fractional Fourier filter to remove noise. The images are then fit to a model involving the known cavity Green’s function in Eq. (S9) and processed to remove nonlocal fields. This leaves an image of the light scattered by each atomic ensemble, the phase and amplitude of which reveals",
      "paper_id": "http://arxiv.org/abs/2509.12202v1"
    },
    {
      "chunk_type": "text",
      "chunk": 20,
      "content": "each atomic ensemble, the phase and amplitude of which reveals each of their spin states ⟨ˆSx i ⟩. For simplicity, we choose to normalize the spin states such that Pn i ⟨ˆSx i ⟩ 2 = 1. Rarely, an atomic ensemble splits into two unequal components that scatter photons with opposite phase. This splitting can reduce the amplitude of ⟨ˆSx i ⟩, which is the average over the atomic ensemble, but the amplitude remains above our noise floor in more than 99% of the cases [9] and thus minimally impacts our measurements. The above analysis is performed for all images shown in the main text. Additionally, the fit results provide the positions ri of the atomic ensembles in the cavity midplane. Slow feedback between trials keeps the array’s CoM position rCoM = Pn i=1 ri/n stabilized to within 0.5 µm of the intended target position. Experimental trials with a rCoM that deviates from the target by more than 1 µm are discarded, which occurs in approximately 8% of trials for our n = 16 neural networks [9]. II. ASSOCIATIVE MEMORY METHODS In this section, we describe how natural memories of the neural networks are found, grouped, and measured for their basin size. The memories are found by cataloging ⟨ˆSx i ⟩spin configurations over many trials with random inputs. The attractors of the spin dynamics are the memory states; thus, by allowing random input states to evolve, we generate a random sample of the memories. With enough random samples, all memories of the neural network can be found with high probability. We use [400, 200, 100, 50] samples per neural network for system sizes n = [16, 12, 8, 4]. A bootstrap analysis of the number of identified memories versus the number of samples is described in Fig. S2. We estimate that for n = [16, 12, 8, 4] we find, on average, [95%, 96%, 96%, 99%] of the total number of memories with basin size ≥1 spin flip at the 50% recall threshold level. It becomes increasingly challenging to find all memories as n increases. Identifying all memories is by no means a requirement to operate the neural network, and we only do so here to study the scaling of the memory capacity. Indeed, the difficulty of finding all memories at higher n highlights the significantly enhanced memory capacity over the Hopfield model. The randomly sampled spin configurations for each",
      "paper_id": "http://arxiv.org/abs/2509.12202v1"
    },
    {
      "chunk_type": "text",
      "chunk": 21,
      "content": "the Hopfield model. The randomly sampled spin configurations for each neural network are organized into a set of memory candidates 11 FIG. S3. Hierarchical clustering of the randomly sampled spin configurations for J1. The dendrogram shows the formation of spin configuration clusters at different levels of the overlap distance dαβ given by Eq. (S1). The upper dashed line at dαβ = 1 cuts the dendrogram at the single-spin-flip level to define memory candidates. Candidates containing a cluster of spin configurations are color-coded red or blue in an alternating fashion for visual clarity; candidates corresponding to single spin configurations remain as black lines below the dαβ = 1 threshold. The lower dashed line shows the two-sigma confidence level above the noise floor. Clusters that form below this level contain all the same spin configuration to within experimental uncertainty. They are recolored green or orange for clarity. See text for details. to be tested. This is accomplished using a hierarchical clustering [39] of the spin states by their overlap distance, dαβ = n 2 \u0012 1 − n X i=1 ⟨ˆSx iα⟩⟨ˆSx iβ⟩ \u0013 , (S1) where α and β label the independently sampled spin configurations. The prefactor n/2 scales the distance such that dαβ is equal to the Hamming distance for spin states with uniform amplitudes. Thus, a distance dαβ = 1 approximately corresponds to one complete spin flip between spin configurations α and β. The hierarchical clustering outputs a grouping of the spin configurations that can be visualized as a tree of related states, or more precisely as a dendrogram. The dendrogram for J1 is shown in Fig. S3, relating to data shown in Fig. 1 and Fig. 2 of the main text. The tree branches into many clusters of similar spin configurations; each vertical line in the dendrogram is one such cluster, and the horizontal lines at branch points show the average dαβ between states in the cluster. The tree further branches into sub-clusters of more closely related states until reaching the leaves at the bottom of the plot, each corresponding to a single spin configuration. This branching arises due to the ultrametric structure of the spin glass we create [9]. Memory candidates are formed by cutting the tree at the upper dashed line where dαβ = 1; each intersection of the upper dashed line with the tree yields a cluster of spin configurations that, on average, differ by",
      "paper_id": "http://arxiv.org/abs/2509.12202v1"
    },
    {
      "chunk_type": "text",
      "chunk": 22,
      "content": "a cluster of spin configurations that, on average, differ by less than one complete spin flip within the cluster. We therefore consider each of these clusters to be a memory candidate, containing a few closely related spin configurations, and color code them alternately red or blue for visual clarity. Spin configurations that are not clustered with any others below the dαβ = 1 cutoff are also considered as candidates and remain as black lines below both cutoffs. The lower dashed line at dαβ = 0.21 is the two-sigma confidence level above the noise floor of our measurements. Clusters that form below this level contain spin configurations that are the same to within experimental uncertainty; these clusters are colored green and orange in an alternating fashion. This grouping procedure for memory candidates is found to enhance the stability of associative memory recall over long durations, combating technical noise and experimental sources of drift, while retaining a unique identity for each memory. Memory candidates are tested for their recall performance to either be deemed a valid memory or ignored. Only the candidates that exhibit a basin size greater than or equal to one spin flip at the 50% recall level are accepted as memories and contribute to the memory capacity of the neural network. See Sec. IV and Sec. V for a discussion of the dependence of memory capacity on the 50% recall threshold. Each candidate is first tested by measuring its recall probability over 30 trials with a stimulus spin configuration that exactly matches the sign signature of the candidate memory; in other words, with zero errors in the input. In the case that the candidate cluster contains a small number of different sign signatures, the one that was found most commonly during random sampling is used as the reference. We find that if the zero-error recall probability is less than 75%, the likelihood of the basin size being greater than or equal to 1 spin flip is very low. Thus, candidates with zero-error recall probability less than 75% are ignored without further testing. Passing candidates are then measured for their basin size under non-zero error in the stimulus. We note that the grouping procedure for memory candidates could inflate the non-zero-error recall probabilities. This is because applying errors to the reference spin configuration could lead to a stimulus that still matches with another 12 spin configuration in the candidate cluster.",
      "paper_id": "http://arxiv.org/abs/2509.12202v1"
    },
    {
      "chunk_type": "text",
      "chunk": 23,
      "content": "matches with another 12 spin configuration in the candidate cluster. However, the effect diminishes with increasing n and occurs in less than 10% of the cases at n = 16 for stimuli with single-spin-flip errors. The effect is negligible for stimuli with two or more errors. Basin sizes are measured over 30 additional recall trials with a variable number of errors. The case of J1 is the exception: We measured full recall curves more thoroughly, conducting 30 trials per error level. But for all other Js, the basin size is estimated in an iterative approach as follows. The number of errors in the first of the 30 additional recall trials is chosen randomly between 1 and n/2. The result is recorded as either a successful or failed recall of the memory at the randomly chosen error level. This data point, combined with the already measured zero-error recall probability, yields a sparse estimate of the memory’s recall curve. The recall curve is fit to produce an estimate of the basin size after one recall trial, b1. Additional recall trials yield a more accurate estimate of the recall curve and thus a more accurate estimate of the basin size. After the first trial, the number of stimulus errors used in the second trial is again randomly sampled between 1 and n/2, but with a bias toward b1. Biasing the number of spin flip errors toward the estimated basin size provides the most information about the true basin size. This is implemented by sampling the number of spin flip errors for trial i + 1 from the probability distribution Pi+1(e) ∝1/[(e −bi)2 + 1.52], where e is the number of errors and bi is the estimated basin size after trial i. We use a distribution width of 1.5 spin flips. The basin size estimate is not sensitive to the precise functional form. This ad-hoc distribution was found to improve the accuracy of the final estimated basin size as compared to uniform random sampling. The second recall trial yields a more accurate estimate of the recall curve and more accurate basin size estimate, b2. This procedure continues over 30 total trials, yielding a final estimate b30 that is the most accurate. Basin sizes are extracted through a least-squares fit of the recall curve to a functional form a1[1 −tanh(a2e −a3)], where a1/2/3 are the fit parameters. The intersection of the fitted curve with the",
      "paper_id": "http://arxiv.org/abs/2509.12202v1"
    },
    {
      "chunk_type": "text",
      "chunk": 24,
      "content": "fit parameters. The intersection of the fitted curve with the 50% recall probability threshold defines the basin size. We find that fitting the recall curves to estimate the basin size is less sensitive to statistical fluctuations than directly interpolating the data to find the 50% crossing point. Though chosen in an ad hoc manner, in Sec. V we show that the tanh functional form fits well to the observed recall curves. Moreover, the estimated basin sizes are insensitive to the precise functional form. The measurement uncertainty in the basin size is estimated via a bootstrap analysis: The 60 recall trials (30 without stimulus errors and 30 with stimulus errors) are resampled with replacement in 100 bootstrap samples, resulting in 100 bootstrapped recall curves. The basin size for each recall curve is estimated through the same tanh fit. The standard deviation in the basin sizes provides an estimate of the uncertainty, which is typically around 0.3 spin flips. The memory capacity for each neural network is determined by counting the number of memories with a basin size greater than or equal to one spin flip. Uncertainty in the basin sizes leads to uncertainty in the memory capacity. This uncertainty in the capacity is estimated through an additional bootstrap analysis. For each neural network, over 1,000 bootstrap samples of each basin size are generated by adding Gaussian-sampled noise to the basin estimate. The noise level is set by the uncertainty in the basin size estimate. Each bootstrap sample contains a noisy estimate of the basin size for each candidate and leads to a single bootstrap estimate of the memory capacity. The mean of the bootstrap distribution is the estimated memory capacity of the neural network that we quote in the main text. The standard deviation of the bootstrap distribution provides the estimated uncertainty in the memory capacity for that neural network. This uncertainty is typically on the scale of ±1 memory. In addition to measuring basin size, a basin volume can also be measured for each memory. The volume of a memory’s basin of attraction is the number of stimulus spin configurations that dynamically evolve to the memory. The volume can be generalized to accommodate stochastic dynamics as follows. Each of the 2n stimulus spin configurations s = (±1, · · · , ±1) have some probability to evolve to a given memory i. If we denote this probability as pi(s), then",
      "paper_id": "http://arxiv.org/abs/2509.12202v1"
    },
    {
      "chunk_type": "text",
      "chunk": 25,
      "content": "memory i. If we denote this probability as pi(s), then the basin volume for memory i is Vi = P s pi(s), where the sum is taken over all 2n spin configurations. Conveniently, the basin volume may be estimated from the random sampling of spin configurations that is initially performed to identify memory candidates. After determining the memories, an estimate of the basin volume is given by 2nNi/Ntotal, where Ni is the number of times memory i is encountered in the random sampling of states and Ntotal is the total number of random samples. Uncertainty in this estimation is computed through bootstrap resampling of the randomly sampled spin configurations. The average basin size at n = 16 is approximately 5,600 spin configurations, with an average uncertainty of 10%. III. THEORETICAL DESCRIPTION This section provides the theoretical framework describing the quantum-optical neural networks we create. We first provide a quantum-optical model that explicitly includes the quantum fields of the multimode cavity. An effective spin model is then derived in which the photonic degrees of freedom are eliminated in the limit of dispersive cavity coupling. We then simplify the description further in a semiclassical model that serves as the basis for the subsequent numerical studies in Sec. IV. 13 A. Quantum-optical model Each associative memory neural network that we create is formed from a network of n atomic ensembles trapped within a multimode optical cavity. The system is driven-dissipative: The ensembles scatter photons from a transverse pump into the cavity, where the photons may scatter from another atomic ensemble, producing a cavity-mediated spin-spin interaction, or dissipate out of the cavity. Each ensemble forms one of the spins in the network, where the collective pseudospin states are formed from motional states of the atomic gas [9]. The motional state in the externally applied trap, without transverse pumping, defines the normal state ψ0 with ⟨ˆSz i ⟩= −S. The atomic wavefunction ψc ∝cos(krz) cos(krx) defines the pseudospin state with ⟨ˆSz i ⟩= S, where kr = 2π/λ. This state arises due to the potential formed from the interference of the transverse pump with the emergent cavity field. Two “checkerboard” density wave states proportional to ψ0 +ψc with ⟨ˆSx i ⟩= S and ψ0 −ψc with ⟨ˆSx i ⟩= −S define what we refer to as the spin up and spin down states, respectively. Each atomic ensemble is thus described by a collective SU(2) spin",
      "paper_id": "http://arxiv.org/abs/2509.12202v1"
    },
    {
      "chunk_type": "text",
      "chunk": 26,
      "content": "atomic ensemble is thus described by a collective SU(2) spin operator ˆSx/y/z i with total spin S = M/2 [9]. The quantum-optical description of the system is provided by a multimode, multi-atomic ensemble generalization of the Hepp-Lieb-Dicke model [9, 47], given by ˆHTotal ℏ = X µ h −∆µˆa† µˆaµ + fµ(eiϕµˆaµ + e−iϕµˆa† µ) i + ωz n X i=1 ˆSz i + X µ n X i=1 gµ(ri) ˆSx i (ˆaµ + ˆa† µ). (S2) The first sum describes the multimode cavity under longitudinal pumping. It includes all the Hermite-Gauss modes Ξµ within one of the near-degenerate mode families of the M/N = 4/7 cavity. The mode labels µ = (l, m) index the number of nodes in the mode profile in the two transverse directions. A mode family contains those Ξµ for which l + m ≡η (mod N) for a fixed integer η ∈[0, N −1]. We employ an η = 0 cavity in this work; see Sec. I for details about the cavity. The operators ˆaµ satisfy canonical bosonic commutation relations [ˆaµ, ˆaν] = [ˆa† µ, ˆa† ν] = 0 and [ˆaµ, ˆa† ν] = δµν. The transverse pump is red-detuned from each mode by ∆µ = ωP −ωµ < 0. We denote the detuning from the fundamental Hermite-Gauss mode Ξ0,0 by ∆C = ∆0,0. A longitudinal pump shaped by a DMD coherently drives the cavity modes with strengths fµ and phases ϕµ. The sum over ˆSz i operators describes the ℏωz ≈2Er energy cost for the formation of the atomic density wave state, where Er ≈h · 3.8 kHz is the atomic recoil energy and h is Planck’s constant. The final sum describes the light-matter interaction between each atomic ensemble and cavity mode. Given the CoM position ri of an atomic ensemble in the cavity midplane, the position-dependent coupling strengths are given by gµ(r) = cos θµ g0Ω ∆A Z dr′ρ(r′ −r)Ξµ(r′), (S3) where g0 = 2π·1.35 MHz is the single-atom, single-photon coupling strength, Ωis the Rabi frequency of the transverse pump, and ∆A ≈−2π · 97 GHz is the excited-state detuning of the transverse pump. The terms Ξµ(r) are spatial mode profiles at the midplane of the cavity and ρ(r) is the overall envelope of the atomic density profile. ρ(r) is well approximated by an isotropic 2D Gaussian distribution with a standard deviation of approximately 5.2 µm. The mode phases θµ simplify to",
      "paper_id": "http://arxiv.org/abs/2509.12202v1"
    },
    {
      "chunk_type": "text",
      "chunk": 27,
      "content": "of approximately 5.2 µm. The mode phases θµ simplify to (l +m)2π/7−(1+Q0)π/2 in the midplane of the η = 0 cavity that we consider, where Q0 is the longitudinal mode number of the Ξ0,0 fundamental mode; see Ref. [9] for the general form of θµ. The atomic density distribution in the cavity midplane ρ(r) is integrated against the Hermite-Gaussian mode profile Ξµ(r) to yield the coupling strength. Dissipation of the cavity modes is captured by the Lindblad master equation d dtρ = −i ℏ[ ˆHTotal, ρ] + κ X µ \u00002ˆaµρˆa† µ −{ˆa† µˆaµ, ρ} \u0001 , (S4) where ρ is the density matrix of the full system. The field decay rate κ = 2π·140 kHz is approximately constant among the cavity modes [35]. B. Effective spin model We now derive a simplified theory in which the photonic degrees of freedom are adiabatically eliminated to yield an effective transverse field Ising model. The position dependence of the spins is retained to allow a description of spin-position coupling in the effective Hamiltonian. We identify this spin-position coupling as the reason for the polaronic deformation of the spin positions noted in the main text. Moreover, we show that the effective model we 14 derive is able to reproduce the enhanced memory capacity due to polaronic deformation. The primary result of this section is to provide an effective theory that plausibly explains the mechanism behind the enhanced memory capacity due to polaronic deformation. We note that more advanced theoretical treatments are necessary to derive an effective model that captures the steepest descent dynamics of the spins, which we performed in Ref. [13], and to derive an effective model that fully captures the form of the dissipation channels, which we performed in Ref. [45]. Deriving a theory that describes all aspects of the spin dynamics in a simplified theory would go well beyond the scope of the current experimental study, but is something that future theoretical work should address. Rather, we focus here on providing a simplified theory that captures the polaronic enhancement to memory capacity. The photonic degrees of freedom are eliminated by inserting the steady-state expressions for the cavity field into the equations of motion for the spin degrees of freedom. This approach captures the effective Hamiltonian seen by the atoms, as discussed below, but does not give a complete description of dissipation due to the retardation of cavity-mediated interactions. The resulting",
      "paper_id": "http://arxiv.org/abs/2509.12202v1"
    },
    {
      "chunk_type": "text",
      "chunk": 28,
      "content": "dissipation due to the retardation of cavity-mediated interactions. The resulting effective model is accurate in the limit of dispersive cavity coupling, |∆C| ≫ωz, κ, |gµ(r)|, and the spread of the near-degenerate cavity modes, which are all approximately satisfied in our experimental parameter regime with ∆C ≈2π · 20 MHz, ωz ≈2π · 7.5 kHz, and |gµ(r)| on the kilohertz scale. The steady-state expressions for the cavity field operators under the Lindblad master equation presented in Eq. S4 are given by ˆaµ = fµe−iϕµ ∆µ + iκ + n X i=1 gµ(ri) ˆSx i ∆µ + iκ . (S5) The steady-state expression above leads to the following Heisenberg equations of motion for the spins after adiabati- cally eliminating the cavity modes, d dt ˆSx i = −ωz ˆSy i , d dt ˆSy i = ωz ˆSx i −f(ri) ˆSz i −g n X j=1 J(ri, rj) \u0000 ˆSz i ˆSx j + ˆSx j ˆSz i \u0001 , d dt ˆSz i = f(ri) ˆSy i + g n X j=1 J(ri, rj) \u0000 ˆSy i ˆSx j + ˆSx j ˆSy i \u0001 , (S6) where g = g2 0Ω2/(∆2 A|∆C|). The terms J(r, r′) and f(r) are position-dependent Ising couplings and longitudinal fields, respectively, and will be described below. The equations of motion above are generated by the following transverse field Ising model, ˆH ℏ= ωz n X i=1 ˆSz i −g n X ij=1 J(ri, rj) ˆSx i ˆSx j − n X i=1 f(ri) ˆSx i . (S7) Equation (S7) above defines the effective spin-position coupling model. We now describe the two position-dependent terms. The first is the Ising coupling matrix, given by J(ri, rj) = ∆2 C X µ Z drdr′ρ(r −ri)ρ(r′ −rj)Ξµ(r)Ξµ(r′) ∆2µ + κ2 = ∆2 C ∆2 C + κ2 Z drdr′ρ(r −ri)ρ(r′ −rj)Gη(r, r′), (S8) where Gη(r, r′) is the cavity Green’s function. This is given by the following expression in the limit of ideal mode degeneracy [9], Gη\u0000r, r′\u0001 = 1 7δ \u0012r −r′ w0 \u0013 + 3 X k=1 1 7π sin(2kπ/7) sin \" (1 + η)2kπ 7 + r2 + r′2 tan(2kπ/7)w2 0 − 2r · r′ sin(2kπ/7)w2 0 # . (S9) We obtain an explicit form of J(ri, rj) using Gaussian atomic densities ρ(r) = exp \u0002 −r2/(2σ2 A) \u0003 /(2πσ2 A). The experi- mental atomic densities may be closely approximated using σA = 5.2 µm. J(ri,",
      "paper_id": "http://arxiv.org/abs/2509.12202v1"
    },
    {
      "chunk_type": "text",
      "chunk": 29,
      "content": "may be closely approximated using σA = 5.2 µm. J(ri, rj) can then be evaluated as [37] J(ri, rj) = ∆2 C ∆2 C + κ2 1 7G′(ri, rj, 1) + 2 7 3 X k=1 Re h e−η2πik/7G′\u0000ri, rj, e2πik/7\u0001i! , (S10) 15 where G′(r, r′, t) is a modified Mehler kernel given by G′(r, r′, t) = (1 + γ)2 4(1 −γ2t2) exp \u0014 − (1 + γ) 2(1 −γ2t2) \u0012 (1 + γt2)(r2 + r′2) w2 0 −2(1 + γ)tr · r′ w2 0 \u0013\u0015 . (S11) Above, γ = (1 −2σ2 A/w2 0)/(1 + 2σ2 A/w2 0) incorporates the finite size of the atomic density. The second position-dependent term is the function f(r). This is the total intracavity field generated by the longitudinal pump integrated against the atomic density, given by f(r) = 2g0Ω ∆A X µ fµ cos \u0002 ϕµ + atan(κ/∆µ) \u0003 q ∆2µ + κ2 Z dr′ρ(r′ −r)Ξµ(r′). (S12) The angle atan(κ/∆µ) is on the order of π/400 in our parameter regime and is negligible. We now evaluate f(r) for the form of the longitudinal pump that we use in recall experiments: A sum of focused Gaussian beams at each position ri with a phase corresponding to a desired stimulus spin configuration si = ±1. Experimental examples of f(r), processed to remove background nonlocal fields [9], are shown in Fig. 1b, Fig. 2a, and Fig. S1a. The input field Ein(r) from the longitudinal pump is more explicitly modeled as Ein(r) = f 2πw2 n X i=1 si exp \u0014 −(r −r0 i )2 w2 \u0015 , (S13) where r0 i are the target positions in the cavity midplane, f is an overall coupling strength and w is the waist of Gaussian input beams. The coupling strengths fµ are then given by the overlap integral with the cavity modes as fµ = R drEin(r)Ξµ(r), and the mode phases are given by a constant ϕµ = ϕ. The experimentally realized waist of the longitudinal pumping beams, w = 7.3 µm, closely matches the shape of the Gaussian atomic distribution with σA ≈5.2 µm: Writing Ein(r) in the form standard Gaussian distributions with standard deviation σ = w/ √ 2 yields σ ≈5.16 µm, closely matching σA. In this case, f(r) can be well approximated by f(r) = cos(ϕ)g0Ωf ∆A p ∆2 C + κ2 X µ n X i=1 si Z dr′dr′′ρ(r′ −r)ρ(r′′ −r0 i",
      "paper_id": "http://arxiv.org/abs/2509.12202v1"
    },
    {
      "chunk_type": "text",
      "chunk": 30,
      "content": "µ n X i=1 si Z dr′dr′′ρ(r′ −r)ρ(r′′ −r0 i )Ξµ(r′)Ξµ(r′′) = cos(ϕ)g0Ωf p ∆2 C + κ2 ∆A∆2 C n X i=1 siJ(r, r0 i ), (S14) where J(r, r′) is the same function that describes the Ising coupling. In writing Eq. (S14), we also make the approxi- mation of ideal mode degeneracy, ∆µ = ∆C for all µ. This approximation becomes accurate in the dispersive coupling regime in which we operate. C. Semiclassical model A semiclassical description of the spin system becomes accurate far above the superradiant threshold when the cavity field enters a coherent state. The resulting model is a simplification that, nevertheless, captures the ability to describe associative memory recall, spin-position coupling, and enhanced memory capacity using numerically tractable equations of motion. The semiclassical model is derived through mean-field decoupling of operator products in the Hamiltonian and equations of motion for the effective spin model. Specifically, the semiclassical energy E is derived by taking the expectation value of Eq. (S7), factorizing product terms, and explicitly including the trap potentials as a function of the CoM coordinates of the atomic ensembles, E = ℏωz n X i=1 ⟨ˆSz i ⟩−ℏg n X ij=1 J(ri, rj)⟨ˆSx i ⟩⟨ˆSx j ⟩−ℏ n X i=1 f(ri)⟨ˆSx i ⟩+ 1 2Etrap n X i=1 \u0012ri −r0 i wt \u00132 , (S15) where wt ≈20 µm is the waist of the external trap potential and Etrap sets the energy scale in the harmonic approximation. Equations of motion for the spin degrees of freedom are derived from Eq. (S6) by taking expectation values and factorizing operator products. We approximate the positional dynamics by damped classical dynamics in the energy landscape. This corresponds to the classical equation of motion md2r dt2 = −dE dr −cd dr dt , (S16) 16 where m is the atomic mass and cd is a damping coefficient. A nonzero cd can arise from, e.g., damping of collective oscillations in the BEC [51]. When the optical forces dE/dr are not too strong, the system is overdamped and the inertial term md2r/dt2 becomes negligible. In this limit, the equation of motion reduces to dr/dt = −(1/cd)dE/dr, corresponding to a gradient descent in the energy landscape. The combined semiclassical equations of motion for the spin and positional degrees of freedom are then given by d dt⟨ˆSx i ⟩= −ωz⟨ˆSy i ⟩, d dt⟨ˆSy i ⟩= ωz⟨ˆSx i ⟩−f(ri)⟨ˆSz i ⟩−2g n X",
      "paper_id": "http://arxiv.org/abs/2509.12202v1"
    },
    {
      "chunk_type": "text",
      "chunk": 31,
      "content": "dt⟨ˆSy i ⟩= ωz⟨ˆSx i ⟩−f(ri)⟨ˆSz i ⟩−2g n X j=1 J(ri, rj)⟨ˆSz i ⟩⟨ˆSx j ⟩, d dt⟨ˆSz i ⟩= f(ri)⟨ˆSy i ⟩+ 2g n X j=1 J(ri, rj)⟨ˆSy i ⟩⟨ˆSx j ⟩, d dtri = −1 cd dE dri . (S17) The above model serves as the backbone for the simulations we perform in Sec. IV C, where we investigate numerical solutions to the above equations of motion. IV. NUMERICAL SIMULATIONS In this section, we perform numerical simulations of associative memory in the Hopfield model, SK model spin glass, and in the quantum-optical neural networks. We first benchmark the memory capacity of the Hopfield and SK models using the same memory criterion applied to our experimental data. We then model associative memory in the quantum-optical neural networks to demonstrate the enhanced capacity arising from polaronic deformations and to estimate the effect of experimental imperfections. A. Hopfield model memory capacity In this section, we numerically determine the memory capacity of the Hopfield model subject to the same criterion that we apply to the determination of capacity in our experimental neural networks: a minimum basin size of one spin flip at the 50% recall level. The Hopfield model is known to have a memory capacity of approximately 0.138 · n in the large-n limit [10]. However, this definition has certain issues: It permits small but extensive deviations from the intended memory patterns [10], does not accurately model small-n effects, and does not enforce a basin size of at least one spin flip. Thus, we numerically compute the Hopfield capacity to allow for direct comparison to our experimentally measured memory capacities presented in the main text. We numerically determine the Hopfield memory capacity as follows. For each n, we optimize the number of intended patterns P to maximize the total number of stored memories. The intended pattern vectors ξp are stored using the standard Hebbian learning rule Jij = PP p=1 ξp i ξp j [3]. This learning rule successfully stores patterns with high probability for small P/N, but as P increases, the probability of successfully storing a pattern decreases; this tradeoff results in an optimal P that stores the most patterns, on average. We determine the optimal P by numerically checking the memory capacity in 10,000 trials with randomly chosen patterns for each value of P. A pattern is considered to be successfully stored if it exhibits greater than 50%",
      "paper_id": "http://arxiv.org/abs/2509.12202v1"
    },
    {
      "chunk_type": "text",
      "chunk": 32,
      "content": "to be successfully stored if it exhibits greater than 50% recall probability under MH dynamics with randomized single-spin-flip errors; this finds all basins of size greater than or equal to one spin flip. The number of stored memories and its standard deviation versus P are shown for n = 16 in Fig. S4a. We find that the maximum number of stored memories is 3.59(1), on average, and this maximum is achieved when intending to store P = 4 memories. This is greater than the 2.21 capacity estimated by the thermodynamic scaling relation 0.138 · n, indicating that there are finite-size effects not captured by the thermodynamic bound. We repeat this procedure for up to n = 50. The average memory capacity and its standard deviation over disorder realizations of random ξp are shown in Fig. S4b. We find the average memory capacity fits well to the form 2.13 + 0.10 · n for n ≳10, as opposed to the thermodynamic estimate 0.138 · n. The offset 2.13 is a finite-size correction that improves the estimate of the memory capacity at small n; the 0.138 · n estimate incorrectly predicts a memory capacity of less than one for n < 8. The scaling 0.10 · n indicates that requiring a minimum basin size of one spin flip and permitting no deviations in the stored pattern reduces the slope of the 0.138 · n scaling estimate for Hopfield capacity. The approximate capacity 2.13 + 0.10 · n is used in Figure 3b of the main text. There, the Hopfield regime is defined by memory capacities bounded by 2.13 + 0.10 · n on the x-axis, and bounded by the maximal average basin 17 FIG. S4. Memory capacity in the Hopfield model when requiring a minimum basin size of one spin flip. a) The average and standard deviation of the number of stored memories for the n = 16 Hopfield model versus the number of intended memory patterns P. Memories must exhibit basin sizes greater than one spin flip at the 50% recall level to be counted in our definition of capacity. The results are averaged over 10,000 random realizations of the J matrix for each P. While the number of stored memories initially tracks the number intended memories, the Hebbian learning rule begins to fail at larger P. This leads to a maximum of approximately 3.59 memories at P = 4. b)",
      "paper_id": "http://arxiv.org/abs/2509.12202v1"
    },
    {
      "chunk_type": "text",
      "chunk": 33,
      "content": "maximum of approximately 3.59 memories at P = 4. b) The average and standard deviation of the number of stored memories in the Hopfield model versus n using the same analysis described in panel (a). A linear fit to the data beginning at n = 10 (to avoid small-n effects) yields an average memory capacity of 2.13 + 0.10 · n. size 2n/(2.13 + 0.10 · n) on the y-axis. These bounds define the shaded area in the plot and approximate the regime in which Hopfield networks can operate. B. SK model spin glass memory capacity A primary result of our work is the demonstration that spin glasses can operate as associative memory neural networks given a suitable form of dynamics. The SK spin glass [40], evolving under SD dynamics, provides a simplified model of our cavity QED system that captures its ability to perform associative memory recall in a spin glass [13]. In this section, we benchmark the recall capability of the SK model to provide a comparison to our experimental data. We have previously shown that the SK model J matrices provide a simple approximation to the disordered J matrices generated by multimode cavity-mediated interactions [9, 13]. The energy of the SK model is given by E = −1 2 Pn i,j Jijsisj, where each si = ±1 is a binary spin variable. Each element Jij of the symmetric coupling matrix is an independent and identically distributed random variable. The Jij are sampled from a Gaussian distribution to yield a single disorder realization of the spin glass. We consider a zero-mean distribution, corresponding to the point that is deepest in the spin glass phase [40]. Each of the exponentially many local minima for a given J are candidate memories. As in both our experimental data and in our analysis of the Hopfield model, we require that a memory demonstrates a basin size of at least one spin flip at the 50% recall level. We consider the SK model evolved with either equilibrium MH dynamics or SD dynamics, as described in the main text. Our previous theoretical work predicts that the SK model with MH dynamics does not function as an associative memory, while the SK model with SD dynamics does. We start by numerically computing the likelihood that a local minimum in the SK model exhibits a basin size greater than one spin flip. This probability is plotted",
      "paper_id": "http://arxiv.org/abs/2509.12202v1"
    },
    {
      "chunk_type": "text",
      "chunk": 34,
      "content": "size greater than one spin flip. This probability is plotted versus n in Fig. S5. We find that for SD dynamics this probability approaches 100% with increasing n, while it monotonically decreases with n (up to a small even/odd n dependence) for MH dynamics. This is consistent with the prediction of Ref. [13] that for large n, all local minima in the SK model become memories under SD dynamics, while none do using MH dynamics. We additionally plot the fraction of memories using a more stringent 75% recall threshold. While the trends are consistent, we note that the asymptotic approach to 100% under SD dynamics occurs more slowly with n. The true cavity QED dynamics are predicted to be closer to SD than MH [13]. We therefore use a 50% recall threshold in the analysis of our experimental data to avoid these more severe finite-size effects. The memory capacity of the SK model under SD and MH dynamics was plotted in Fig. 3, where the capacity under MH dynamics appears to bend upwards. However, the basin size of memories under MH dynamics is predicted to decrease to zero with increasing n, while a nearly extensive basin size is predicted for SD [13]. Thus, the memory capacity under MH may eventually plateau or begin to decrease at larger n. Seeing this potential turnaround in the MH capacity falls outside the current capabilities of our numerical methods but is not required for the current study. 18 FIG. S5. Fraction of local minima that are associative memories in the SK model spin glass. Each data point is averaged over at least 200 realizations of SK-model J matrices. For each J, local minima are found by relaxing up to 54,000 random initial spin states to local minima. Standard error is smaller than the marker in all cases. Blue (red) lines show the fraction of minima that are memories using SD (MH) dynamics. Solid (dashed) lines show the results requiring a minimum basin size of one spin flip at the 50% (75%) recall level. Rather, the simulated SK memory capacity at the experimentally realized system sizes up to n = 16 provide a direct comparison point for the experimentally measured capacities in Fig. 3 of the main text. C. Simulation of quantum-optical neural networks We now simulate associative memory recall in quantum-optical neural networks using the semiclassical model presented in Sec. III C. The semiclassical",
      "paper_id": "http://arxiv.org/abs/2509.12202v1"
    },
    {
      "chunk_type": "text",
      "chunk": 35,
      "content": "the semiclassical model presented in Sec. III C. The semiclassical model we study yields estimates of the memory capacity and its enhancement due to polaronic effects that are consistent with our experimental data. A single recall trial is simulated by numerically solving the equations of motion in Eq. (S17) using an 8th order Runge-Kutta method. The spins are all initialized in the normal state, ⟨ˆSz i ⟩= −S and ⟨ˆ Sx i ⟩= ⟨ˆ Sy i ⟩= 0. The CoM positions ri are initialized at their respective trap locations r0 i . The parameter ωz is set to match the experimental value provided in the main text. We use a damping coefficient cd = ℏ/(0.002 µm2), corresponding to a damping ratio of approximately 1,000 for our trap frequencies. This puts the positional dynamics deep into the overdamped regime, ensuring the gradient descent approximation to the classical position dynamics is accurate. The results that follow are insensitive to the precise value of cd. The trap energy Etrap and trap width wt are set so that the average magnitude of position deviations due to spin-position coupling matches the experimental value of approximately 1.3 µm. The transverse and longitudinal pump schedules used in the simulations are shown in Fig. S6a, which are designed to closely approximate the experimentally realized ramp schedule shown in Fig. S1. The transverse pump is exponentially ramped to 4gc, matching the experimental ramp form, but does not include the final quench to ∼4.5gc for imaging. That brief 300 µs quench is not expected to affect the spin configuration. The longitudinal field strengths |fi| are ramped in the simulation using a simple analytic approximation to the experimentally realized schedule in Fig. S1b, which is complicated by experimental constraints. The simulation uses a cosine ramp from zero at time t = 0 to |fi| = 0.5ωz| cos(ϕi)| at time t = 1.6 ms, where ϕi is the relative phase between the transverse pump and the longitudinal pump beam focused onto spin i. This relative phase will be discussed further below. The amplitude 0.5ωz is set to approximately match the experimentally realized amplitude. The |fi| are then held constant until t = 5 ms when they ramp down to zero with another cosine functional form finishing at t = 7 ms. This leaves 1 ms of spin evolution time in the absence of longitudinal pumping before measuring the spin state at t",
      "paper_id": "http://arxiv.org/abs/2509.12202v1"
    },
    {
      "chunk_type": "text",
      "chunk": 36,
      "content": "of longitudinal pumping before measuring the spin state at t = 8 ms, matching the experimental conditions. Figure S6 shows an example simulation of recall dynamics using trap locations r0 i that correspond to the J1 neural network discussed in the main text. The stimulus pattern is a corrupted memory state, with randomly chosen spin 19 FIG. S6. Simulation of a single recall trial. a) The transverse pump strength (blue) and longitudinal pump strength (orange) follow schedules approximating the experimental sequence shown in Fig. S1. b) Evolution of the x components of the position deviations ri −r0 i = (xi −x0 i , yi −y0 i ). c) Evolution of the y components of the position deviations. d-f) Time evolution of the ⟨ˆSx/y/z i ⟩spin components. g) Evolution of the Ising energy −ℏg Pn ij=1 J(ri, rj)⟨ˆSx i ⟩⟨ˆSx j ⟩, the energy due to the longitudinal fields −ℏPn i=1 f(ri)⟨ˆSx i ⟩, the energy due to the transverse field ℏωz Pn i=1 ⟨ˆSz i ⟩, and the trap energy Etrap Pn i=1(ri −r0 i )2/(2w2 t ). The energies are normalized by S. The Ising energy reaches approximately −60ωz by t = 8 ms. flip errors applied to spins 3, 4, and 5. The spin evolution is shown in Fig. S6d-f. The ⟨ˆSx i ⟩components are initially biased toward the stimulus fields fi. However, as the transverse pump strength increases and as the stimulus fields are ramped off, spins 3, 4, and 5 flip in sign before all the spins approach a polarized ⟨ˆSx i ⟩spin configuration. These spin flips are the recall process in action; the initially corrupted spins are corrected by the neural network dynamics. Because only the initially corrupted spins flip, this is an example of a successful memory recall trial. Summarizing the evolution of the other spin components, the ⟨ˆSz i ⟩are initially polarized toward −S in the normal state, but approach zero as spin organization occurs. The ⟨ˆSy i ⟩components show little evolution during recall. The BEC positions deviate from their trap minima due to the spin-position coupling terms in Eq. (S15). The deviations ri−r0 i are shown in Fig. S6b,c. The gradient descent position dynamics minimize the total energy, including the trap energy as well as that from the spin-position coupling terms −ℏg P ij J(ri, rj)⟨ˆSx i ⟩⟨ˆSx j ⟩and −ℏP i f(ri)⟨ˆSx i ⟩. The positions continue to evolve up to",
      "paper_id": "http://arxiv.org/abs/2509.12202v1"
    },
    {
      "chunk_type": "text",
      "chunk": 37,
      "content": "f(ri)⟨ˆSx i ⟩. The positions continue to evolve up to t = 8 ms because the energy landscape continues to shift due to the increasing transverse pump strength. The positions stabilize when the transverse pump strength is held constant (not shown). Figure S6g shows the evolution of the terms in the semiclassical energy. At t = 0, the spins are in the normal state and the transverse term ωz P i ⟨ˆSz i ⟩dominates the energy. As the longitudinal fields ramp up, it becomes energetically favorable for the ⟨ˆSx i ⟩to become nonzero to match the stimulus pattern. At later times, transverse pumping makes the Ising interaction dominant, and the ⟨ˆSx i ⟩organize more deeply and possibly undergo spin flips to correct initially corrupted spins in the stimulus. The trap energy begins to increase at later times, when the BECs begin to deviate significantly from their trap locations to further reduce the energy of the Ising term through spin-position coupling. Memories are found in the simulated system using the same technique that is used experimentally: Spin states are cataloged using >400 trials with randomly chosen stimuli. After identifying candidate memories, we test each for its recall performance. Figure S7 shows a typical simulated recall curve using one such memory candidate for J1, with 100 trials per error level. We are able to include in our simulations the two dominant sources of experimental noise that can inhibit recall: trial-to-trial fluctuations in the stimulus and tweezer trap locations. As mentioned above in Sec. I, other sources of experimental noise, such as positional drift of the longitudinal pump beams and global atom number fluctuations, are not severe enough to have a strong effect on recall performance in our parameter 20 FIG. S7. A typical simulated recall curve for the J1 neural network. Each data point is averaged over 100 recall trials. The recall curve is simulated using both the experimental level of stimulus and trap noise as well as without these noise terms. regime. Fluctuations in the tweezer traps are modeled by adding Gaussian noise with standard deviation 0.5 µm to the trap locations r0 i before each recall trial. This corresponds to the experimental level of stability. Fluctuations in the stimulus consists of 10% intensity fluctuations between different fi, an uncontrolled phase ϕtrans between the longitudinal and transverse pump beam, and 0.3 radian phase fluctuations between the fi, as described in Sec.",
      "paper_id": "http://arxiv.org/abs/2509.12202v1"
    },
    {
      "chunk_type": "text",
      "chunk": 38,
      "content": "radian phase fluctuations between the fi, as described in Sec. I. This means that for each recall trial, the nominal fi are replaced with Aifi cos(ϕtrans + ϕi) where the Ai are Gaussian normal variables with mean 1 and standard deviation 0.1, ϕtrans is sampled randomly from [0, 2π], and the ϕi are sampled from zero-mean Gaussian distributions with 0.3-radian standard deviation. Figure S7 shows the simulated recall curve including the experimentally measured levels of stimulus and trap noise, as well as the recall curve with these noise terms artificially turned off. The zero-error recall probability always begins at 100% in the absence of noise, but noise can reduce it. Most, but not all of the experimental memory candidates have a zero-error recall probability below 100% due to this effect; see Fig. S8 for examples. Similarly, the noise can shrink the basin size. The severity of the reduction in recall probability varies among memories. However, the basin sizes are typically reduced more when using higher recall cutoffs for the basin size. The 50% recall threshold we use in the main text is low enough to mitigate these effects of experimental noise. We numerically simulate the J1 memory capacity in the same way that it is measured experimentally, by counting the number of memories with a basin size greater than or equal to 1 spin flip. We use the same bootstrap analysis described in Sec. II to estimate the error. The memory capacity is simulated using three levels of trap strengths to control the level of polaronic elasticity in the positions: the default experimental trap strength, the 4× weaker trap strength that experimentally yields enhanced memory capacity, and the limit of strong traps, for which the elastic response of the atoms is completely eliminated. Moreover, for each trap strength we simulate the memory capacity using the experimental levels of stimulus and trap noise, only stimulus noise, only trap noise, and with no noise. The results are summarized in Table I. We find that the simulated memory capacity matches the experimentally measured capacities for both the default and enhanced polaronic elasticity settings, within error. This suggests that the semiclassical theory may be an accurate predictor of the memory capacity, including polaronic enhancement, despite its inability to accurately reproduce the specific memory states themselves; this is discussed below. The simulations indicate that the memory capacity could be enhanced up to threefold by suppressing",
      "paper_id": "http://arxiv.org/abs/2509.12202v1"
    },
    {
      "chunk_type": "text",
      "chunk": 39,
      "content": "memory capacity could be enhanced up to threefold by suppressing experimental noise sources. For example, the simulated capacity at default experimental conditions increases from 12(2) with both noise sources to 36(2) without them. In future work, fluctuations in the stimulus could be mitigated by phase locking the longitudinal and transverse pump beams. Replacing the DMD with a spatial light modulator would additionally offer a greater degree of tunability and improved power efficiency, leading to stronger and more homogeneous fi. Trap position fluctuations likely result from differential movement between the cavity and optical table. This could be mitigated by, e.g., locking the tweezer positions to a fixed point in the cavity’s frame of reference. The simulations additionally reveal elasticity to be a defense mechanism against J chaos arising from position fluctuations. Fluctuations in the trap locations directly lead to fluctuations in the J matrix through the position dependence described by Eq. (S8). The J fluctuations, in turn, can negatively impact recall performance. For example, in the absence of elastic effects, the simulated capacity drops from 22(1) without noise to 14(1) when including only trap noise; the decreased capacity is due to J chaos. However, elasticity makes the spins less sensitive 21 Experimental capacity Simulated capacity (stimulus + trap noise) Simulated capacity (stimulus noise) Simulated capacity (trap noise) Simulated capacity (no noise) Strong trap limit (no elasticity) — 10(1) 11(2) 14(1) 22(1) Default trap strength (default elasticity) 14(1) 12(2) 17(2) 23(2) 36(2) 4× weaker trap (enhanced elasticity) 25(2) 23(3) 27(3) 60(4) 60(4) TABLE I. Simulated J1 memory capacity under different conditions. Each row corresponds to a different level of polaronic elasticity, as controlled by the trap strength. No elasticity corresponds to the limit of strong traps. The first column shows the experimentally measured capacities shown in Fig. 3 in the main text. The second column shows the capacity simulated including the two dominant noise sources affecting recall, trial-to-trial fluctuations in the trap positions and stimulus field. The remaining columns show the capacities simulated using only stimulus noise, only trap noise, and without any noise. to initial position misplacement by giving them the ability to move and correct their positions. The simulated capacity with enhanced elastic effects does not drop at all upon including trap noise, remaining 60(4) in both the noise-free case and when including only trap noise. Overall, the simulations reveal elasticity as a mechanism to defend against J chaos by",
      "paper_id": "http://arxiv.org/abs/2509.12202v1"
    },
    {
      "chunk_type": "text",
      "chunk": 40,
      "content": "elasticity as a mechanism to defend against J chaos by making J a dynamical quantity. This could play an important role in preserving recall performance in larger-scale spin glass associative memories. We note that the semiclassical model is not accurate enough to predict the memory states themselves with high accuracy. To quantify the agreement, we compare the simulated memories with those found experimentally. For each simulated memory, we find the experimental memory that best matches, or in other words, differs by the least number of spin flips. For J1, the simulated memories contain an average of approximately six spin flip errors compared to its closest-matching experimental memory. The discrepancy arises most likely because of imprecision in our analytic estimate of J(ri, rj), which does not capture experimental details such as imperfect mode degeneracy [37]. A precise match to the experimental memories is not needed for simulating the memory capacity and polaronic effects, which appear to be captured well by the semiclassical theory. V. EXTENDED DATA This section presents additional data supporting the conclusions in the main text. We present here a more extensive characterization of the J1 neural network, an analysis of memory capacities using a more strict 75% recall threshold, and a demonstration of associative memory in an n = 20 neural network. A. J1 memory candidates The J1 neural network pertaining to Fig. 2 was characterized more fully than the others. The recall curve was measured using 30 trials per error level for each memory candidate that demonstrated at least 75% zero-error recall probability. For other neural networks, we employ the faster basin estimation algorithm described in Sec. II. We present these J1 recall curves below and show that they fit well to a tanh functional form. Figure S8 shows extended recall data for J1. The J1 matrix is shown explicitly in Fig. S8a and is computed via Jij = J(r0 i , r0 j) using Eq. (S8). We evaluate Jij using the trap locations r0 i , corresponding to the J matrix before spin-position coupling begins to dynamically modify J. The J matrix is seen to fluctuate randomly in sign and strength between different spin pairs. This generates frustrated spin interactions, yielding a spin glass [9]. The diagonal elements are always positive and extend beyond the range of the color bar, reaching an average value of approximately 0.7. These diagonal interactions induce a collective superradiant emission within",
      "paper_id": "http://arxiv.org/abs/2509.12202v1"
    },
    {
      "chunk_type": "text",
      "chunk": 41,
      "content": "0.7. These diagonal interactions induce a collective superradiant emission within the same atomic ensemble associated with the BEC adopting one or the other density wave pattern; in other words, these interactions encourage the effective spins within the same atomic ensemble to align. Antidiagonal matrix elements correspond to pairs of spins that are approximately at mirror image locations reflected through the cavity center. This can lead to stronger coupling strengths due to the cavity Green’s function shown in Eq. (S9), but does not significantly affect the random nature of the coupling matrix. Figure S8 additionally shows each of the 20 memory candidates and their recall curves. The candidates are ordered according to how frequently they were encountered during the initial sampling with random stimuli. Each recall curve is fit to the functional form a1[1 −tanh(a2x −a3)] to estimate the basin size at the 50% recall level. The average 22 FIG. S8. All J1 memory candidates with zero-error recall probability greater than 75%. a) Calculated J1 matrix using Eq. (S8), demonstrating effectively random, sign-changing interactions. b1-20) Representative images for each of the twenty memory candidates. Each image is normalized to its maximum amplitude. The black bar in each image shows w0 = 34.8 µm. c1-20) Recall curves for each memory candidate, fit to the functional form a1[1 −tanh(a2x −a3)]. reduced chi-squared statistic for the tanh fit is 1.17, indicating consistency between the tanh form and the measured data. For example, memory candidate 1 in Fig. S8 has a basin size that exceeds our one-spin-flip threshold and corresponds to memory 1 in the main text. Similarly, memory candidates 5-7 in Fig. S8 are labeled in the main text as memories 2-4, respectively. B. Memory capacity using 75% recall threshold We use a recall threshold of 50% to measure the basin size and determine the memory capacities presented in the main text. This threshold is a free parameter; the minimum acceptable recall probability may depend on the specific application of the neural network. However, we show in Sec. IV B that higher recall thresholds yield more severe finite-size effects that reduce memory capacity. We also show in Sec. IV C that using higher recall thresholds makes the memory capacity more susceptible to experimental noise sources. We thus use a 50% threshold because it mitigates finite-size effects and reduces the impact of experimental noise while still being high enough to be considered a reasonable threshold",
      "paper_id": "http://arxiv.org/abs/2509.12202v1"
    },
    {
      "chunk_type": "text",
      "chunk": 42,
      "content": "still being high enough to be considered a reasonable threshold for acceptable recall performance. Nevertheless, we present the memory capacities using a more 23 FIG. S9. Experimental memory capacities using a 75% recall threshold. The capacity for each experimentally realized neural network is shown with a blue circle and error bar. The n = 16 neural network with enhanced polaronic elasticity is shown with a gray triangle. The arrow points from the J1 neural network to its more elastic version. The experimental average using the 75% recall threshold is shown using green diamonds. The average capacity with the 50% recall threshold used in the main text is reproduced using red diamonds for comparison. The average simulated capacity of the SK model using SD (MH) dynamics is shown with a blue (orange) line. strict 75% threshold for all measured neural networks in this section. Memory capacities using a 75% recall threshold are determined in the same manner as for the 50% threshold. The basin size is estimated for each candidate memory, but the basin size now corresponds to the intersection of the recall curve with the 75% recall threshold, leading to smaller basin sizes. The memory capacity is still the number of memories with a basin size greater than or equal to one spin flip. Error bars are determined via the same bootstrap analysis described in Sec. II. The results are shown in Fig. S9. As expected, the capacity drops across all n. The average capacity at n = 16 drops from 11.9(6) to 5(1). The neural network with enhanced polaronic effects still yields an increase in capacity, but the increase is now smaller. The decrease in memory capacity using the 75% threshold is most severe for the more elastic neural network. This is because it has the most memory candidates with a basin size near one spin flip, and so it loses the most memories when enforcing the more stringent recall requirement. The memory capacity in the SK model is also reduced when using the 75% threshold. This is computed numerically in the same way as described in Sec. IV B but using a 75% recall threshold. The average memory capacity is plotted for both SD and MH dynamics in Fig. S9 for comparison with the experimental neural networks. The average experimental capacity remains roughly consistent with the SK model under SD dynamics; the lower capacity at n = 16",
      "paper_id": "http://arxiv.org/abs/2509.12202v1"
    },
    {
      "chunk_type": "text",
      "chunk": 43,
      "content": "under SD dynamics; the lower capacity at n = 16 may be due to statistical fluctuations or experimental noise not present in the SK model simulations. Overall, while the memory capacity decreases using a 75% recall threshold, this decrease is as expected and is due to finite-size effects and increased susceptibility to experimental noise. The distinction remains with respect to the Hopfield capacity. C. A 20-site neural network We present an initial demonstration of associative memory in an n = 20 neural network. The experimental system is able to realize neural networks with n = 20 spin ensembles and perform recall sequences as described in Sec. I. However, power limitations on the longitudinal pump lead to a 10% reduction in the stimulus strength |fi|. This is a minor effect, and so the same experimental recall procedure is retained for n = 20. Full characterization of the memory capacity becomes prohibitively time-consuming at n = 20 due to the rapid increase in the number of memory candidates. We encountered 97 memory candidates after performing approximately 1,100 trials with random stimuli. The recall performance was measured for the 25 candidates that were found most frequently in the random sampling. Of the 25 candidates we tested, 11(1) are assessed to possess basin sizes greater than or equal to one spin flip and therefore satisfy our criterion as memories. The top 12 memory candidates are shown in Fig. S10 along with their estimated basin size using the algorithm described in Sec. II. Thus, the memory capacity of this n = 20 neural network is bounded from below by 11(1) but is likely much higher, since only 25 of the 97 memory candidates were tested. Properly characterizing the abundance of memories in neural networks with n > 16 requires methods beyond the brute-force searching and testing methods employed in this work. Future 24 FIG. S10. Memories in an n = 20 neural network. a) The J matrix as computed using Eq. (S8). The diagonal elements extend beyond the range of the color bar and have an average value of approximately 0.5. Antidiagonal matrix elements correspond to spins at approximately mirror image locations in the cavity and have slightly stronger coupling strengths, as seen in Fig. S8a for the J1 neural network. b-m) The first 12 memory candidates found with basin sizes close to or greater than one spin flip. Basin sizes estimated using the algorithm",
      "paper_id": "http://arxiv.org/abs/2509.12202v1"
    },
    {
      "chunk_type": "text",
      "chunk": 44,
      "content": "than one spin flip. Basin sizes estimated using the algorithm described in Sec. II. work could explore stochastic sampling of memories, or implement multiple recall trials per experimental sequence, to better investigate associative memory in larger-scale spin glasses.",
      "paper_id": "http://arxiv.org/abs/2509.12202v1"
    }
  ],
  "id_to_indices": {
    "http://arxiv.org/abs/2509.12202v1": [
      0,
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9,
      10,
      11,
      12,
      13,
      14,
      15,
      16,
      17,
      18,
      19,
      20,
      21,
      22,
      23,
      24,
      25,
      26,
      27,
      28,
      29,
      30,
      31,
      32,
      33,
      34,
      35,
      36,
      37,
      38,
      39,
      40,
      41,
      42,
      43,
      44
    ]
  }
}