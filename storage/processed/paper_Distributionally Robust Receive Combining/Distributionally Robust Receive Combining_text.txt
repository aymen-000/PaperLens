arXiv:2401.12345v3  [eess.SP]  17 Jun 2025
1
Distributionally Robust Receive Combining
Shixiong Wang, Wei Dai, and Geoffrey Ye Li, Fellow, IEEE
Abstract—This article investigates signal estimation in wireless
transmission (i.e., receive combining) from the perspective of
statistical machine learning, where the transmit signals may be
from an integrated sensing and communication system; that is,
1) signals may be not only discrete constellation points but also
arbitrary complex values; 2) signals may be spatially correlated.
Particular attention is paid to handling various uncertainties
such as the uncertainty of the transmit signal covariance, the
uncertainty of the channel matrix, the uncertainty of the channel
noise covariance, the existence of channel impulse noises, the non-
ideality of the power amplifiers, and the limited sample size of
pilots. To proceed, a distributionally robust receive combining
framework that is insensitive to the above uncertainties is
proposed, which reveals that channel estimation is not a necessary
operation. For optimal linear estimation, the proposed framework
includes several existing combiners as special cases such as diag-
onal loading and eigenvalue thresholding. For optimal nonlinear
estimation, estimators are limited in reproducing kernel Hilbert
spaces and neural network function spaces, and corresponding
uncertainty-aware solutions (e.g., kernelized diagonal loading)
are derived. In addition, we prove that the ridge and kernel
ridge regression methods in machine learning are distributionally
robust against diagonal perturbation in feature covariance.
Index Terms—Wireless Transmission, Smart Antenna, Ma-
chine Learning, Robust Estimation, Robust Combining, Distri-
butional Uncertainty, Channel Uncertainty, Limited Pilot.
I. INTRODUCTION
I
N wireless transmission, detection and estimation of trans-
mitted signals is of high importance, and combining at
array receivers serves as a key signal-processing technique to
suppress interference and environmental noises. The earliest
beamforming solutions rely on the use of phase shifters (e.g.,
phased arrays) to steer and shape wave lobes, while advanced
combining methods allow the employment of digital signal
processing units, which introduce additional structural freedom
(e.g., fully digital, hybrid, nonlinear, wideband) in combiner
design and significant performance improvement in signal
recovery [1]–[3].
In traditional communication systems, transmitted signals
are discrete points from constellations. Therefore, signal re-
covery, commonly referred to as signal detection, can be cast
into a classification problem from the perspective of statistical
machine learning, and the number of candidate classes is
determined by the number of points in the employed constel-
lation. Research in this stream includes, e.g., [4]–[9] as well
as references therein, and the performance measure for signal
detection is usually the misclassification rate (i.e., symbol error
S. Wang, W. Dai, and G. Li are with the Department of Electrical
and Electronic Engineering, Imperial College London, London SW7 2AZ,
United Kingdom (E-mail: s.wang@u.nus.edu; wei.dai1@imperial.ac.uk; ge-
offrey.li@imperial.ac.uk).
This work is supported by the UK Department for Science, Innovation
and Technology under the Future Open Networks Research Challenge project
TUDOR (Towards Ubiquitous 3D Open Resilient Network).
rate); representative algorithms encompass the maximum like-
lihood detector, the sphere decoding, etc. In another research
stream, the signal recovery performance is evaluated using
mean-squared errors (cf., signal-to-interference-plus-noise ra-
tio), and the resultant signal recovery problem is commonly
known as signal estimation, which can be considered as a
regression problem from the perspective of statistical machine
learning. By comparing the estimated symbols with the con-
stellation points afterward, the detection of discrete symbols
can be realized. For this case, till now, typical combining solu-
tions include zero-forcing receivers, Wiener receivers (i.e., lin-
ear minimum mean-squared error receivers), Capon receivers
(i.e., minimum variance distortionless response receivers), and
nonlinear receivers such as neural-network receivers [10]–[12].
On the basis of these canonical approaches, variants such
as robust beamformers working against the limited size of
pilot samples and the uncertainty in steering vectors [13]–
[18] have also been intensively reported; among these robust
solutions, the diagonal loading method [19], [14, Eq. (11)]
and the eigenvalue thresholding method [20], [14, Eq. (12)]
are popular due to their excellent balance between practical
performance and technical simplicity.
Different from traditional paradigms, in emerging commu-
nication systems, e.g., integrated sensing and communication
(ISAC) systems, transmitted signals may be arbitrary complex
values and spatially correlated [21]–[23]. As a result, mean-
squared error is a preferred performance measure to investigate
the receive combining and estimation problem of wireless
signals, which is, therefore, the focus of this article.
Although a large body of problems have been attacked in the
area, the following signal-processing problems of combining
and estimation in wireless transmission remain unsolved.
1) What is the relation between the signal-model-based
approaches (e.g., Wiener and Capon receivers) and the
data-driven approaches (e.g., deep-learning receivers)? In
other words, how can we build a mathematically unified
modeling framework to interpret all the existing digital
receive combiners?
2) In addition to the limited pilot size and the uncertainty
in steering vectors, there exist other uncertainties in
the signal model: the uncertainty of the transmit signal
covariance, the uncertainty of the communication channel
matrix, the uncertainty of the channel noise covariance,
the presence of channel impulse noises (i.e., outliers), and
the non-ideality of the power amplifiers. Therefore, how
can we handle all these types of uncertainties in a unified
solution framework?
3) Existing literature mainly studied the robustness theory
of linear beamformers against limited pilot size and the
uncertainty in steering vectors [13]–[18]. However, how
can we develop the theory of robust nonlinear combiners

2
against all the aforementioned uncertainties?
To this end, this article designs a unified modeling and
solution framework for receive combining of wireless signals,
in consideration of the scarcity of the pilot data and the
different uncertainties in the signal model.
A. Contributions
The contributions of this article can be summarized from the
aspects of machine learning theory and wireless transmission
theory.
In terms of machine learning theory, we give a justification
of the popular ridge regression and kernel ridge regression
(i.e., quadratic loss function plus squared-F-norm regulariza-
tion) from the perspective of distributional robustness against
diagonal perturbation in feature covariance, which enriches the
theory of trustworthy machine learning; see Theorems 2 and
3, as well as Corollaries 3 and 5.
In terms of wireless transmission theory, the contributions
are outlined below.
1) We build a fundamentally theoretical framework for re-
ceive combining from the perspective of statistical ma-
chine learning. In addition to the linear estimation meth-
ods, nonlinear approaches (i.e., nonlinear combining) are
also discussed in reproducing kernel Hilbert spaces and
neural network function spaces. In particular, we reveal
that channel estimation is not a necessary operation in
receive combining. For details, see Subsection III-A.
2) The presented framework is particularly developed from
the perspective of distributional robustness which can
therefore combat the limited size of pilot data and several
types of uncertainties in the wireless signal model such
as the uncertainty in the transmit power matrix, the
uncertainty in the communication channel matrix, the
existence of channel impulse noises (i.e., outliers), the
uncertainty in the covariance matrix of channel noises,
the non-ideality of the power amplifiers, etc. For details,
see Subsection III-B, and the technical developments in
Sections IV and V.
3) Existing methods such as diagonal loading and eigen-
value thresholding are proven to be distributionally robust
against the limited pilot size and all the aforementioned
uncertainties in the wireless signal model. Extensions of
diagonal loading and eigenvalue thresholding are pro-
posed as well. Moreover, the kernelized diagonal loading
and the kernelized eigenvalue thresholding methods are
put forward for nonlinear estimation cases. For details,
see Corollary 1, Examples 4 and 5, and Subsections IV-B.
4) The distributionally robust receive combining and sig-
nal estimation problems across multiple frames, where
channel conditions may change, are also investigated. For
details, see Subsections IV-C and V-A2.
B. Notations
The N-dimensional real (coordinate) space and complex
(coordinate) space are denoted as RN and CN, respectively.
Lowercase symbols (e.g., x) denote vectors (column by de-
fault) and uppercase ones (e.g., X) denote matrices. We use
the Roman font for random quantities (e.g., x, X) and the italic
font for deterministic quantities (e.g., x, X). Let Re X be the
real part of a complex quantity X (a vector or matrix) and
Im X be the imaginary part of X. For a vector x ∈CN, let
x :=

Re x
Im x

∈R2N
be the real-space representation of x; for a matrix H ∈
CN×M, let
H :=

Re H
Im H

,
H :=

Re H
−Im H
Im H
Re H

be the real-space representations of H where H ∈R2N×M
and H ∈R2N×2M. The running index set induced by an
integer N is defined as [N] := {1, 2, . . . , N}. To concate-
nate matrices and vectors, MATLAB notations are used: i.e.,
[A, B] for row stacking and [A; B] for column stacking.
We let ΓM := [IM, JM] ∈CM×2M where IM denotes
the M-dimensional identity matrix, JM
:= j · IM, and
j denotes the imaginary unit. Let N(µ, Σ) denote a real
Gaussian distribution with mean µ and covariance Σ. We use
CN(s, P , C) to denote a complex Gaussian distribution with
mean s, covariance P , and pseudo-covariance C; if C is not
specified, we imply C = 0.
II. PRELIMINARIES
We review two popular structured representation methods
of nonlinear functions ϕ : RN →RM. More details can be
seen in Appendix A.
A. Reproducing Kernel Hilbert Spaces
A reproducing kernel Hilbert space (RKHS) H induced by
the kernel function ker : RN × RN →R and a collection of
points {x1, x2, . . . , xL} ⊂RN is a set of functions from RN
to R; L may be infinite. Every function ϕ : RN →R in the
functional space H can be represented by a linear combination
[24, p. 539; Chap. 14]
ϕ(x) =
L
X
i=1
ωi · ker(x, xi), ∀x ∈RN
(1)
where {ωi}i∈[L] are the combination weights; ωi ∈R for every
i ∈[L]. The matrix form of (1) for M-multiple functions are
ϕ(x) :=


ϕ1(x)
ϕ2(x)
...
ϕM(x)

= W · φ(x) :=


ω1
ω2
...
ωM

· φ(x), (2)
where ω1, ω2, . . . , ωM
∈RL are weight row-vectors for
functions ϕ1(x), ϕ2(x), . . . , ϕM(x), respectively, and
W :=


ω1
ω2
...
ωM

∈RM×L,
φ(x) :=


ker(x, x1)
ker(x, x2)
...
ker(x, xL)

. (3)
Since a kernel function is pre-designed (i.e., fixed) for an
RKHS H, (2) suggests a W -linear representation of x-
nonlinear functions ϕ(x) in HM. Note that there exists a

3
one-to-one correspondence between ϕ and W : for every
ϕ : RN →RM, there exists a W ∈RM×L, and vice versa.
B. Neural Networks
Neural networks (NN) are another powerful tool to represent
(i.e., approximate) nonlinear functions. A neural network func-
tion space (NNFS) K characterizes (or parameterizes) a set of
multi-input multi-output functions. Typical choices are multi-
layer feed-forward neural networks, recurrent neural networks,
etc. For combining and estimation of wireless signals, the
multi-layer feed-forward neural networks are standard [10]–
[12]. Suppose that we have R −1 hidden layers (so in total
R + 1 layers including one input layer and one output layer)
and each layer r = 0, 1, . . . , R contains Tr neurons. To
represent a function ϕ : RN →RM, for the input layer r = 0
and output layer r = R, we have T0 = N and TR = M,
respectively. Let the output of the rth layer be yr ∈RTr. For
every layer r, we have yr = σr(W ◦
r · yr−1 + br) where
W ◦
r
∈RTr×Tr−1 is the weight matrix, br ∈RTr is the
bias vector, and the multi-output function σr is the activation
function which is entry-wise identical. Hence, every function
ϕ : RN →RM in a NNFS can be recursively expressed as
[25, Chap. 5], [26]
ϕ(x)
= σR(WR · [yR−1(x); 1])
yr(x)
= σr(Wr · [yr−1(x); 1]),
r ∈[R −1]
y0(x)
= x,
(4)
where Wr := [W ◦
r , br] for r ∈[R]. Note that the activation
functions can vary from one layer to another.
III. PROBLEM FORMULATION
Consider a narrow-band wireless signal transmission model
x = Hs + v
(5)
where x ∈CN is the received signal, s ∈CM is the transmit-
ted signal, H ∈CN×M is the channel matrix, and v ∈CN
is the zero-mean channel noise. The precoding operation (if
exists) is integrated in H. The transmitted symbols s have
zero means, which may be not only discrete symbols from
constellations such as quadrature amplitude modulation but
also arbitrary values such as integrated sensing and commu-
nication signals. We consider L pilots S := (s1, s2, . . . , sL)
in each frame, and the corresponding received symbols are
X := (x1, x2, . . . , xL) under the noise (v1, v2, . . . , vL). We
suppose that Rs := EssH and Rv := EvvH may not be
identity or diagonal matrices: i.e., the components of s can be
correlated (e.g., in ISAC), so can be these of v. Consider the
real-space representation of the signal model (5) by stacking
the real and imaginary components:
x = H · s + v,
(6)
where x ∈R2N, H ∈R2N×2M, s ∈R2M, and v ∈R2N.
The expressions of Rx := ExxT, Rs := EssT, Rxs := ExsT,
and Rv := EvvT can be readily obtained; see Appendix B. In
some cases, signal estimation in real spaces can be technically
simpler than that in complex spaces.
A. Optimal Estimation
1) Optimal Nonlinear Estimation (Receive Combining):
To recover s using x, we consider an estimator ˆs := ϕ(x),
called a receive combiner, at the receiver where ϕ : CN →
CM is a Borel-measurable function. Note that ϕ(x) may be
nonlinear in general because the joint distribution of (x, s) is
not necessarily Gaussian, for example, when the channel noise
v is non-Gaussian or when the power amplifiers work in non-
linear regions. The signal estimation problem at the receiver
can be written as a statistical machine-learning problem under
the joint data distribution Px,s of (x, s), that is,
min
ϕ∈BCN →CM Tr Ex,s[ϕ(x) −s][ϕ(x) −s]H,
(7)
where BCN→CM
contains all Borel-measurable estimators
from CN to CM. In what follows, we omit the notational
dependence on CN and CM, and use B as a shorthand. The
optimal estimator, in the sense of minimum mean-squared
error, is known as the conditional mean of s given x, i.e.,
ˆs = ϕ(x) = E(s|x).
(8)
Usually, it is computationally complicated to find the optimal
ϕ(·) from the whole space B of Borel-measurable functions,
that is, to compute the conditional mean. Therefore, in prac-
tice, we may find the optimal approximation of ϕ(·) in an
RKHS H or a NNFS K; note that H and K are two subspaces
of B. However, both H and K are sufficiently rich because they
can be dense in the space of all continuous bounded functions.
2) Optimal Linear Estimation (Receive Beamforming): If
x and s are jointly Gaussian (e.g., when s and v are jointly
Gaussian), the optimal estimator ϕ is linear in x:
ˆs = W x,
(9)
where W ∈CM×N is called a receive beamformer or a linear
receive combiner. In this linear case, (7) reduces to the usual
Wiener–Hopf beamforming problem
min
W Tr Ex,s[W x −s][W x −s]H,
(10)
that is,
min
W Tr

W RxW H −W Rxs −RH
xsW H + Rs

,
(11)
where Rx := ExxH ∈CN×N and Rxs := ExsH ∈CN×M.
Since Rx = HRsHH + Rv and Rxs = HRs + EvsH =
HRs, the solution of (11), or (10), is
W ⋆
Wiener
= RH
xsR−1
x
= RsHH[HRsHH + Rv]−1,
(12)
which is known as the Wiener beamformer. With an additional
constraint W H = IM (i.e., distortionless response), (11)
gives the Capon beamformer. Both the Wiener beamformer
and the Capon beamformer maximize the output signal–to–
interference-plus-noise ratio (SINR); hence, both are optimal
in the sense of maximum output SINR.
No matter whether Px,s is Gaussian or not, (10) or (11)
identifies the optimal linear estimator in the sense of mini-
mum mean-squared error among all linear estimators.

4
3) Role of Channel Estimation: Eqs. (7) and (10) imply
that channel estimation is not a necessary step in receive
combining. The only necessary element, from the perspective
of statistical machine learning, is the joint distribution Px,s of
the received signal x and the transmitted signal s. Therefore,
the following two points can be highlighted.
a) If the joint distribution Px,s is non-Gaussian, we just need
to learn the mapping ϕ using (7).
b) If the joint distribution Px,s is (or assumed to be) Gaussian,
we just learn covariance matrices Rxs and Rx; cf. (12);
Gaussianity assumption of Px,s is beneficial in reducing
computational burdens. If, further, the channel matrix H
is known, Rxs and Rx can be expressed using H.
B. Distributional Uncertainty and Distributional Robustness
For ease of conceptual illustration, we start with the fol-
lowing stationary-channel assumption in this subsection: The
channel statistics remain unchanged within the communication
frame so that the joint distribution Px,s is fixed over time.
That is, pilot data {(x1, s1), (x2, s2), . . . , (xL, sL)} and non-
pilot communication data are drawn from the same unknown
distribution Px,s. For the general case where the channel is
not statistically stationary within a frame, see Appendix C;
the statistical non-stationarity of Px,s may be due to the time-
selectivity of the transmit power matrix Rs, of the channel
matrix H, and/or of the channel noise covariance Rv.
1) Issue of Distributional Uncertainty: In practice, the true
joint distribution Px,s is unknown but can be estimated by the
pilot data. Hence, the estimation of wireless signals is a data-
driven statistical inference (i.e., statistical machine learning)
problem. We let
ˆPx,s := 1
L
L
X
i=1
δ(xi,si)
(13)
denote the empirical distribution supported on the L collected
data {(xi, si)}i∈[L], where δ(xi,si) denotes the Dirac distri-
bution (i.e., point-mass distribution) centered on (xi, si); note
that ˆPx,s is a discrete distribution. If we use the estimated joint
distribution ˆPx,s as a surrogate of the true joint distribution
Px,s, (7) becomes the conventional empirical risk minimiza-
tion (ERM)
min
ϕ∈B Tr E(x,s)∼ˆPx,s[ϕ(x) −s][ϕ(x) −s]H,
(14)
i.e.,
min
ϕ∈B Tr 1
L
L
X
i=1
[ϕ(xi) −si][ϕ(xi) −si]H.
(15)
Likewise, (11) become the conventional beamforming problem
min
W Tr

W ˆRxW H −W ˆRxs −ˆRH
xsW H + ˆRs

,
(16)
where ˆRx, ˆRxs, and ˆRs are the training-sample-estimated
(i.e., nominal) values of Rx, Rxs, and Rs, respectively.
There exists the distributional difference between the
sample-defined nominal distribution
ˆPx,s and true data-
generating distribution Px,s due to the limited size of the
training data set (i.e., limited pilot length) and the time-
selectivity of Px,s. From the perspective of applied statistics
and machine learning, the distributional difference between
ˆPx,s and Px,s (i.e., the distributional uncertainty of ˆPx,s com-
pared to Px,s) may cause significant performance degradation
of (15) compared to (7), so is the performance deterioration of
(16) compared to (11). For extensive reading on this point, see
Appendix C. Therefore, to reduce the adverse effect introduced
by the distributional uncertainty in ˆPx,s, a new surrogate of
(7) rather than the sample-averaged approximation in (15) is
expected.
2) Distributionally Robust Estimation: To combat the dis-
tributional uncertainty in ˆPx,s, we consider the distributionally
robust counterpart of (7)
min
ϕ∈B
max
Px,s∈Ux,s Tr Ex,s[ϕ(x) −s][ϕ(x) −s]H,
(17)
where Ux,s, called a distributional uncertainty set, contains
a collection of distributions that are close to the nominal
distribution (i.e., the sample-estimated distribution) ˆPx,s;
Ux,s := {Px,s| d(Px,s, ˆPx,s) ≤ϵ},
(18)
where d(·, ·) denotes a similarity measure (e.g., metric or di-
vergence) between two distributions and ϵ ≥0 an uncertainty
quantification level. Since ˆPx,s is discrete and Px,s is not,
the Wasserstein distance [27, Def. 2] and the maximum mean
discrepancy (MMD) distance [28, Def. 2.1] are the typical
choices of d(·, ·) to construct Ux,s. When ˆPx,s and Px,s are
parametric distributions (e.g., Gaussian, exponential family),
divergences such as the Kullback-–Leibler (KL) divergence, or
more general ϕ-divergence, are also applicable to particularize
d(·, ·) because parameters can be estimated using samples.
When ϵ = 0, (17) reduces to (15).
If Ux,s contains (or is assumed, for computational simplicity,
to contain) only Gaussian distributions, (17) particularizes to
min
W max
R
Tr

W RxW H −W Rxs −RH
xsW H + Rs

s.t.
d0(R,
ˆR) ≤ϵ0,
R ⪰0,
(19)
where
R :=
 Rx
Rxs
RH
xs
Rs

,
ˆR :=

ˆRx
ˆRxs
ˆRH
xs
ˆRs

,
(20)
because every zero-mean complex Gaussian distribution
is uniquely characterized by its covariance and pseudo-
covariance, but in receive beamforming, we do not consider
pseudo-covariances; cf. (12); d0 denotes the matrix similarity
measures (e.g., matrix distances); ϵ0 ≥0 is the uncertainty
quantification parameter. When ϵ0 = 0, (19) reduces to (16).
For additional discussions on the framework of distribution-
ally robust estimation, see Appendix D.
IV. DISTRIBUTIONALLY ROBUST LINEAR ESTIMATION
Due to several practical benefits of linear estimation, for
example, the simplicity of hardware structures, the clarity of
physical meaning (i.e., constructive and destructive interfer-
ence through beamforming), and the easiness of computations,
investigating distributionally robust linear estimation problems
is important. This section particularly studies Problem (19).

5
A. General Framework and Concrete Examples
The following lemma solves Problem (19).
Lemma 1: Suppose that the set {R| d0(R,
ˆR) ≤ϵ0}
is compact convex and Rx is invertible. Let R⋆solve the
problem below:
max
R
Tr

−RH
xsR−1
x Rxs + Rs

s.t.
d0(R,
ˆR) ≤ϵ0,
R ⪰0,
Rx ≻0.
(21)
Construct W ⋆using R⋆as follows:
W ⋆:= R⋆H
xs R⋆−1
x
.
(22)
Then (W ⋆, R⋆) is a solution to Problem (19). On the other
hand, if (W ⋆, R⋆) solves Problem (19), then R⋆is a solution
to (21) and (W ⋆, R⋆) satisfies (22).
Proof: See Appendix E.
□
Let
f1(R) := Tr

−RH
xsR−1
x Rxs + Rs

(23)
denote the objective function of (21). When Rs and Rxs are
fixed, we define
f2(Rx) := Tr

−RH
xsR−1
x Rxs + Rs

.
(24)
The theorem below studies the properties of f1 and f2.
Theorem 1: Consider the definition of R in (20). The
functions f1 defined in (23) and f2 defined in (24) are
monotonically increasing in R and Rx, respectively. To be
specific, if R1 ⪰R2 ⪰0, R1,x ≻0, and R2,x ≻0, we have
f1(R1) ≥f1(R2). In addition, if R1,x ⪰R2,x ≻0, we have
f2(R1,x) ≥f2(R2,x).
Proof: See Appendix F.
□
To concretely solve (21), we need to particularize d0. This
article investigates the following uncertainty sets.
Definition 1 (Additive Moment Uncertainty Set): The addi-
tive moment uncertainty set of R is constructed as
{R| ˆR −ϵ0E ⪯R ⪯ˆR + ϵ0E, R ⪰0}
(25)
for some E ⪰0 and ϵ0 ≥0.
□
Definition 1 is motivated by the fact that the difference
R −ˆR is bounded by some threshold matrix E and error
quantification level ϵ0: specifically, −ϵ0E ⪯R −ˆR ⪯ϵ0E.
In practice, we can consider the threshold as an identity
matrix because, for every non-identity E ⪰0, we have
E ⪯λ1IN+M where λ1 is the largest eigenvalue of E.
Definition 2 (Diagonal-Loading Uncertainty Set): The
diagonal-loading uncertainty set of R is constructed as
{R| ˆR −ϵ0IN+M ⪯R ⪯ˆR + ϵ0IN+M, R ⪰0}
(26)
for some ϵ0 ≥0.
□
Due to the concentration property of the sample-covariance
ˆR to the true covariance R when the true distribution Px,s is
fixed within a frame, finite values of ϵ0 exist for every sample
size L; NB: ϵ0 →0 as L →∞. However, given L, the smallest
ϵ0 cannot be practically calculated because it depends on the
true but unknown Px,s. If E is block-diagonal, the generalized
diagonal-loading uncertainty set can be motivated.
Definition 3 (Generalized Diagonal-Loading Uncertainty
Set): The generalized diagonal-loading uncertainty set of R
is constructed by the following constraints: R ⪰0 and

ˆRx
ˆRxs
ˆRH
xs
ˆRs

−ϵ0
 F
0
0
G

⪯
 Rx
Rxs
RH
xs
Rs

⪯

ˆRx
ˆRxs
ˆRH
xs
ˆRs

+ ϵ0
 F
0
0
G

,
(27)
for some F , G ⪰0 and ϵ0 ≥0.
□
Definitions 1, 2, and 3 are introduced for the first time in
this article. Another type of moment-based uncertainty set is
popular in the literature, which we refer to as the multiplicative
moment uncertainty set for differentiation.
Definition 4 (Multiplicative Moment Uncertainty Set [29]):
The multiplicative moment uncertainty set of R is given as
{R| θ1 ˆR ⪯R ⪯θ2 ˆR}
(28)
for some θ2 ≥1 ≥θ1 ≥0.
□
The following corollary shows the distributionally robust
linear beamformers associated with the various uncertainty sets
in Definitions 1, 2, 3, and 4.
Corollary 1 (of Theorem 1): Consider the moment-based
uncertainty sets in Definitions 1, 2, 3, and 4. The distribution-
ally robust linear beamforming (21) is analytically solved by
the corresponding upper bounds of R. To be specific,
C1) Under Definition 1, the additive-moment distribution-
ally robust (DR-AM) beamformer is
W ⋆
DR-AM
= ( ˆRxs + ϵ0Exs)H( ˆRx + ϵ0Ex)−1
= ( ˆ
H ˆRs + ϵ0Exs)H·
[ ˆ
H ˆRs ˆ
HH + ˆRv + ϵ0Ex]−1,
(29)
where ˆ
H, ˆRs, and ˆRv denote the estimates of H, Rs,
and Rv, respectively.
C2) Under Definition 2, the diagonal-loading distribution-
ally robust (DR-DL) beamformer is
W ⋆
DR-DL
= ˆRH
xs[ ˆRx + ϵ0IN]−1
= ˆRs ˆ
HH[ ˆ
H ˆRs ˆ
HH + ˆRv + ϵ0IN]−1,
(30)
which is also known as the loaded sample matrix
inversion method [19], [14, Eq. (11)] and widely-used
in the practice of wireless communications.
C3) Under Definition 3, the generalized diagonal-loading
distributionally robust beamformer (DR-GDL) is
W ⋆
DR-GDL
= ˆRH
xs[ ˆRx + ϵ0F ]−1
= ˆRs ˆ
HH[ ˆ
H ˆRs ˆ
HH + ˆRv + ϵ0F ]−1.
(31)
C4) Under Definition 4, the multiplicative-moment (MM)
distributionally robust beamformer is identical to the
Wiener beamformer (12) at nominal values:
W ⋆
DR-MM
= ˆRH
xs ˆR−1
x
= ˆRs ˆ
HH[ ˆ
H ˆRs ˆ
HH + ˆRv]−1.
(32)
The corresponding estimation errors are simple to obtain. □

6
Corollary 1 implies that, in the sense of the same induced
robust beamformers, the diagonal-loading uncertainty set (26)
and the generalized diagonal-loading uncertainty set (27) are
technically equivalent to the following trimmed versions.
Definition 5 (Trimmed Diagonal-Loading Uncertainty Sets):
By setting G := 0 in (27), in terms of Rx, (27) reduces to
the trimmed generalized diagonal-loading uncertainty set:
{Rx| ˆRx −ϵ0F ⪯Rx ⪯ˆRx + ϵ0F , Rx ⪰0}.
(33)
The trimmed diagonal-loading uncertainty set
{Rx| ˆRx −ϵ0IN ⪯Rx ⪯ˆRx + ϵ0IN, Rx ⪰0},
(34)
is obtained by letting F := IN.
□
The robust beamformers corresponding to the trimmed
uncertainty sets (33) and (34) remain the same as defined in
(31) and (30), respectively; cf. Theorem 1.
As we can see from Corollary 1, the primary benefit of
using the moment-based uncertainty sets is the computational
simplicity due to the availability of closed-form solutions.
If the uncertainty sets are constructed using the Wasserstein
distance
q
Tr[R + ˆR −2( ˆR1/2R ˆR1/2)1/2] ≤ϵ0 or the KL
divergence
1
2[Tr[ ˆR−1R −IN+M] −ln det( ˆR−1R)] ≤ϵ0
between CN(0, R) and CN(0, ˆR), the induced distribution-
ally robust linear beamforming problems have no closed-form
solutions, and therefore, are computationally prohibitive in
practice. In addition, Corollary 1 suggests that the distribu-
tionally robust beamformer under the multiplicative moment
uncertainty set (28) is the same as the nominal beamformer
ˆRH
xs ˆR−1
x , which essentially does not introduce robustness in
wireless signal estimation; this is another motivation why we
construct new moment-based uncertainty sets in Definitions 1,
2, and 3. However, we can modify the multiplicative moment
uncertainty set in Definition 4 to achieve robustness.
Definition 6 (Modified Multiplicative Moment Uncertainty
Set): The modified multiplicative moment uncertainty set of
R is defined by the following constraint:
 θ1 ˆRx
ˆRxs
ˆRH
xs
θ1 ˆRs

⪯
 Rx
Rxs
RH
xs
Rs

⪯
 θ2 ˆRx
ˆRxs
ˆRH
xs
θ2 ˆRs

(35)
for some θ2 ≥1 ≥θ1 ≥0 such that the left-most matrix is
positive semi-definite.
□
The robust beamformer under the modified multiplicative
moment uncertainty set (35) is
W ⋆
DR-MMM = ˆRH
xs · [θ2 ˆRx]−1.
(36)
In terms of the uncertainties of Rs and Rv, Problem (21)
can be explicitly written as
max
Rs,Rv
Tr

Rs −RsHH(HRsHH + Rv)−1HRs

s.t.
d1(Rs, ˆRs) ≤ϵ1,
d2(Rv, ˆRv) ≤ϵ2,
Rs ⪰0, Rv ⪰0,
(37)
for some similarity measures d1 and d2 and nonnegative
scalars ϵ1 and ϵ2. For every given (Rs, Rv), the associated
beamformer is given in (12). When the uncertainty in the
channel matrix must be investigated, we can consider
max
H
Tr

Rs −RsHH(HRsHH + Rv)−1HRs

s.t.
d3(H, ˆ
H) ≤ϵ3,
(38)
which is not a semi-definite program. In addition, the gradient
of the objective function with respect to H is complicated to
obtain. Hence, practically, we should avoid directly attacking
Problem (38); this can be done by directly considering the
uncertainties of Rx and Rxs (i.e., R) because the uncertainties
of Rs, Rv, and H can be reflected in the uncertainties of Rx
and Rxs; cf. Rx = HRsHH + Rv and Rxs = HRs.
In addition to Corollary 1, below we provide other concrete
examples to further showcase the usefulness and applications
of the distributionally robust beamforming formulations (21)
and (37), where the trimmed uncertainty sets are employed.
Example 1 (Distributionally Robust Capon Beamforming):
We consider a distributionally robust Capon beamforming
problem under the trimmed uncertainty set (34):
min
W max
Rx
Tr

W RxW H −2Rs + Rs

s.t.
W H = IM,
ˆRx −ϵ0IN ⪯Rx ⪯ˆRx + ϵ0IN,
Rx ⪰0,
which is equivalent, in the sense of the same solutions, to
min
W max
Rx
Tr

W RxW H
s.t.
W H = IM,
ˆRx −ϵ0IN ⪯Rx ⪯ˆRx + ϵ0IN,
Rx ⪰0.
According to Theorem 1, the above display is equivalent to
min
W
Tr

W ˆRxW H
+ ϵ0 · Tr

W W H
s.t.
W H = IM.
The above formulation is the squared-F-norm–regularized
Capon beamformer [14, Eq. (10)] whose solution is
W ⋆
DR-Capon = [HH( ˆRx + ϵ0IN)−1H]−1·
HH( ˆRx + ϵ0IN)−1,
(39)
which is the diagonal-loading Capon beamformer.
□
Example 2 (Eigenvalue Thresholding): Suppose that
ˆRx
admits the eigenvalues of diag{λ1, λ2, . . . , λN} in descending
order and the eigenvectors in Q (columns). Let 0 ≤µ ≤1 be
a shrinking coefficient. If we assume Rx ⪯ˆRx,thr where
ˆRx,thr :=
Q


λ1
max{µλ1, λ2}
...
max{µλ1, λN}

Q−1,
(40)
we have the distributionally robust beamformer
W ⋆
DR-ET = RH
xs ˆR−1
x,thr,
(41)
which is known as the eigenvalue thresholding method [20],
[14, Eq. (12)].
□

7
Example 3 (Distributionally Robust Beamforming for Un-
certain Rs and Rv): Consider Problem (37). Since the objec-
tive of (37) is increasing in both Rs and Rv,1 if
ˆRs −ϵ1IM ⪯Rs ⪯ˆRs + ϵ1IM,
we have a distributionally robust beamformer
W ⋆
DR
= ( ˆRs + ϵ1IM)HH[H( ˆRs + ϵ1IM)HH + Rv]−1
= ( ˆRs + ϵ1IM)HH[H ˆRsHH + Rv + ϵ1HHH]−1;
(42)
if instead
ˆRs −ϵ1HH(HHH)−2H ⪯Rs ⪯ˆRs +ϵ1HH(HHH)−2H,
(43)
we have
W ⋆
DR = [ ˆRsHH + ϵ1HH(HHH)−1]·
[H ˆRsHH + Rv + ϵ1IN]−1,
(44)
which is a modified diagonal-loading beamformer. On the
other hand, if
ˆRv −ϵ2IN ⪯Rv ⪯ˆRv + ϵ2IN,
we have
W ⋆
DR = RsHH[HRsHH + ˆRv + ϵ2IN]−1,
(45)
which is also a diagonal-loading beamformer.
□
Motivated by Corollary 1 and Examples 1∼3, as well as the
trimmed uncertainty sets in Definition 5, we have the following
important theorem, which justifies the popular ridge regression
in machine learning.
Theorem 2 (Ridge Regression and Tikhonov Regularization):
Consider a linear regression problem on (x, s), i.e.,
s = W x + e,
where e denotes the error term and the distributionally robust
estimator of W , i.e.,
min
W ∈CM×N
max
Px,s∈Ux,s Tr Ex,s[W x −s][W x −s]H,
which can be particularized to (19). Supposing that the second-
order moment of x is uncertain and quantified as
ˆRx −ϵ0IN ⪯Rx ⪯ˆRx + ϵ0IN,
then the distributionally robust estimator of W becomes a
ridge regression (i.e., squared-F-norm regularized) method
min
W Tr

W ˆRxW H−W ˆRxs−ˆRH
xsW H+ ˆRs

+ϵ0 Tr

W W H
.
The regularization term becomes Tr

W F W H
, which is
known as the Tikhonov regularizer, if
ˆRx −ϵ0F ⪯Rx ⪯ˆRx + ϵ0F
for some F ⪰0.
Proof: This is due to Lemma 1 and Theorem 1. Just note
that Tr

W ( ˆRx + ϵ0F )W H −W ˆRxs −ˆRH
xsW H + ˆRs

=
Tr

W ˆRxW H−W ˆRxs−ˆRH
xsW H+ ˆRs

+ϵ0 Tr

W F W H
.
This completes the proof.
□
1This claim can be routinely proven in analogy to Theorem 1 and a real-
space case in [30, Thm. 1].
Note that in Theorem 2, the second-order moment of s is not
considered because it does not influence the optimal solution
of W : i.e., the optimal solution of W does not depend on the
value of Rs. Theorem 2 gives a new theoretical interpretation
of the popular ridge regression in machine learning from
the perspective of distributional robustness against second-
moment uncertainties of the feature vector x; another interpre-
tation of ridge regression from the perspective of distributional
robustness under martingale constraints is identified in [31,
Ex. 3.3]. When the uncertainty is quantified by the Wasserstein
distance, a similar result can be seen in [32, Prop. 3], [33,
Prop. 2], which however is not a ridge regression formulation
because in [32, Prop. 3] and [33, Prop. 2], the loss function
is square-rooted and the norm regularizer is not squared; cf.
also [27, Rem. 18 and 19]. The corollary below justifies the
rationale of any norm-regularized method.
Corollary 2: The following squared-norm-regularized beam-
forming formulation can combat the distributional uncertainty:
min
W Tr

W ˆRxW H −W ˆRxs −ˆRH
xsW H + ˆRs

+ λ∥W ∥2,
(46)
where ∥·∥denotes any matrix norm. This is because all norms
on CM×N are equivalent; hence, there exists some λ ≥0 such
that λ∥W ∥2 ≥ϵ0∥W ∥2
F = ϵ0 Tr

W W H
. As a result, (46)
can upper bound the ridge cost in Theorem 2.
□
Motivated by Theorem 2, the following corollary is imme-
diate, which gives another interpretation of ridge regression
and Tikhonov regularization from the perspective of data
augmentation through data perturbation (cf. noise injection in
image [34] and speech [35] processing).
Corollary 3 (Data Augmentation for Linear Regression):
Consider a linear regression problem on (x, s) with data
perturbation vectors (∆x, ∆s)
(s + ∆s) = W (x + ∆x) + e,
and the distributionally robust estimator of W
min
W ∈CM×N
max
P∆x,∆s∈U∆x,∆s
Tr E(x,s)∼ˆPx,sE∆x,∆s
n
[W (x + ∆x) −(s + ∆s)][W (x + ∆x) −(s + ∆s)]Ho
.
Suppose that ∆x is uncorrelated with x, with s, and with ∆s;
in addition, ∆s is uncorrelated with x. If the second-order
moment of ∆x is upper bounded as E∆x∆H
x ⪯ϵ0IN, then
the distributionally robust estimator of W becomes a ridge
regression (i.e., squared-F-norm regularized) method
min
W Tr

W ˆRxW H−W ˆRxs−ˆRH
xsW H+ ˆRs

+ϵ0 Tr

W W H
.
The regularization term becomes Tr

W F W H
, which is
known as the Tikhonov regularizer, if E∆x∆H
x ⪯ϵ0F , for
some F ⪰0.
□
The second-order moment of ∆s is not considered in
Corollary 3 as it does not influence the optimal value of W .
B. Complex Uncertainty Sets
Below we remark on more general construction methods for
the uncertainty set of R using the Wasserstein distance and
the F-norm, beyond the moment-based methods in Definitions

8
1∼6. However, note that such complicated approaches are
computationally prohibitive in practice when N or M is large.
1) Wasserstein Distributionally Robust Beamforming: We
start with the Wasserstein distance:
max
R
Tr

−RH
xsR−1
x Rxs + Rs

s.t.
Tr
h
R + ˆR −2( ˆR1/2R ˆR1/2)1/2i
≤ϵ2
0
R ⪰0, Rx ≻0.
(47)
The first constraint in the above display is a particularization of
the Wasserstein distance between CN(0, R) and CN(0, ˆR).
Problem (47) is a nonlinear positive semi-definite program
(P-SDP). However, we can give it a linear reformulation.
Proposition 1: Problem (47) can be equivalently reformu-
lated into a linear P-SDP
max
R,V ,U
Tr[Rs −V ]
s.t.

V
RH
xs
Rxs
Rx

⪰0
Tr
h
R + ˆR −2U
i
≤ϵ2
0
 ˆR1/2R ˆR1/2
U
U
IN+M

⪰0
R ⪰0, Rx ≻0, V ⪰0, U ⪰0.
(48)
Proof: This is by applying the Schur complement.
□
Complex-valued linear P-SDP can be solved using, e.g., the
YALMIP solver.2
Suppose that R⋆solves (48). The corresponding Wasser-
stein distributionally robust beamformer is given as
W ⋆
DR-Wasserstein = R⋆H
xs R⋆−1
x
.
(49)
Next, we separately investigate the uncertainties in ˆRs and
ˆRv. From (37), we have
max
Rs,Rv
Tr

Rs −RsHH(HRsHH + Rv)−1HRs

s.t.
Tr
h
Rs + ˆRs −2( ˆR1/2
s
Rs ˆR1/2
s
)1/2i
≤ϵ2
1
Tr
h
Rv + ˆRv −2( ˆR1/2
v
Rv ˆR1/2
v
)1/2i
≤ϵ2
2
Rs ⪰0, Rv ⪰0,
(50)
where we ignore the uncertainty of H for technical tractability.
Problem (50) can be transformed into a linear P-SDP using a
similar technique as in Proposition 1. One can just introduce
an inequality U ⪰RsHH(HRsHH + R)−1HRs and the
objective function will become Tr [Rs −U].
Suppose that (R⋆
s, R⋆
v) solves (50). The corresponding
Wasserstein distributionally robust beamformer is given as
W ⋆
DR-Wasserstein-Individual = R⋆
sHH[HR⋆
sHH + R⋆
v]−1.
(51)
2) F-Norm Distributionally Robust Beamforming: Under
the F-norm, we just need to replace the Wasserstein distance.
To be specific, (47) becomes
max
R
Tr

−RH
xsR−1
x Rxs + Rs

s.t.
Tr
h
(R −ˆR)H(R −ˆR)
i
≤ϵ2
0
R ⪰0, Rx ≻0.
(52)
2See https://yalmip.github.io/inside/complexproblems/.
The linear reformulation of the above display is given in
the proposition below.
Proposition 2: The nonlinear P-SDP (52) can be equiva-
lently reformulated into a linear P-SDP
max
R,V ,U
Tr[Rs −V ]
s.t.

V
RH
xs
Rxs
Rx

⪰0
Tr [U] ≤ϵ2
0,

U
(R −ˆR)H
(R −ˆR)
IN+M

⪰0,
R ⪰0, Rx ≻0, V ⪰0, U ⪰0.
(53)
Proof: This is by applying the Schur complement.
□
C. Multi-Frame Case: Dynamic Channel Evolution
Each frame contains a pilot block used for beamformer
design. Although the channel state information (CSI) may
change from one frame to another, the CSI between the two
consecutive frames is highly correlated. This correlation can
benefit beamformer design across multiple frames. Suppose
that {(s1, x1), (s2, x2), . . . , (sL, xL)} is the training data
in the current frame and {(s′
1, x′
1), (s′
2, x′
2), . . . , (s′
L, x′
L)}
is the history data in the immediately preceding frame. In
such a case, the distributional difference between ˆPx,s and
ˆPx′,s′ is upper bounded, that is, d(ˆPx,s, ˆPx′,s′) ≤ϵ′ for
some proper distance d and a real number ϵ′ ≥0 where
ˆPx,s := 1
L
PL
i=1 δ(xi,si) and ˆPx′,s′ := 1
L
PL
i=1 δ(x′
i,s′
i).
Since a beamformer W = F(Px,s) is a continuous func-
tional F(·) of data distribution Px,s, cf. (10), we have ∥W −
W ′∥F = ∥F(ˆPx,s)−F(ˆPx′,s′)∥F ≤C·d(ˆPx,s, ˆPx′,s′) ≤ϵ for
some positive constant C ≥0 and upper bound ϵ ≥0 where
W ′ is the beamformer associated with ˆPx′,s′ in the previous
frame. Therefore, the beamforming problem (11) becomes a
constrained problem
min
W
Tr

W RxW H −W Rxs −RH
xsW H + Rs

s.t.
Tr[W −W ′][W −W ′]H ≤ϵ2.
By the Lagrange duality theory, it is equivalent to
min
W Tr

W RxW H −W Rxs −RH
xsW H + Rs

+
λ · Tr[W −W ′][W −W ′]H
= min
W Tr

W (Rx + λIN)W H −W (Rxs + λW ′H)−
(Rxs + λW ′H)HW H + (Rs + λW ′W ′H)

,
(54)
for some λ ≥0. As a result, we have the Wiener beamformer
for the multi-frame case, where we can treat W ′ as a prior
knowledge of W .
Claim 1 (Multi-Frame Beamforming): The Wiener beam-
former for the multi-frame case is given by
W ⋆
Wiener-MF
= [Rxs + λW ′H]H[Rx + λIN]−1
= [RsHH + λW ′][HRsHH + Rv + λIN]−1,
(55)
where λ ≥0 is a tuning parameter to control the similarity
between W and W ′. Specifically, if λ is large, W must be
close to W ′; if λ is small, W can be far away from W ′. □

9
With the result in Claim 1, (21) becomes
max
R
Tr

−(Rxs + λW ′H)H · (Rx + λIN)−1·
(Rxs + λW ′H) + (Rs + λW ′W ′H)

s.t.
d0(R, ˆR) ≤ϵ0,
R ⪰0,
(56)
whose objective function is monotonically increasing in R.
The remaining distributional robustness modeling and anal-
yses against the uncertainties in R are technically straight-
forward, and therefore, we omit them here. Upon using
the diagonal-loading method on R, a distributionally robust
beamformer for the multi-frame case is
W ⋆
DR-Wiener-MF = [ ˆRxs + λW ′H]H · [ ˆRx + λIN + ϵ0IN]−1,
where ϵ0 is an uncertainty quantification parameter for R.
V. DISTRIBUTIONALLY ROBUST NONLINEAR ESTIMATION
For the convenience of the technical treatment, we study
the estimation problem in real spaces. Nonlinear estimators,
which are suitable for non-Gaussian Px,s, are to be limited in
reproducing kernel Hilbert spaces and feedforward multi-layer
neural network function spaces.
A. Reproducing Kernel Hilbert Spaces
1) General Framework and Concrete Examples:
As a
standard treatment in machine learning, we use the par-
tial pilot data {x1, x2, . . . , xL} to construct the reproduc-
ing kernel Hilbert spaces, and use the whole pilot data
{(x1, s1), (x2, s2), . . . , (xL, sL)} to train the optimal estima-
tor in an RKHS.
With the W -linear representation of ϕ(·) in (2), i.e., ϕ(·) =
W φ(·), the distributionally robust estimation problem (17)
becomes
min
W ∈R2M×L
max
Px,s∈Ux,s Tr Ex,s[W · φ(x) −s][W · φ(x) −s]T.
(57)
The proposition below reformulates and solves (57).
Proposition 3: Let K denote the kernel matrix associated
with the kernel function ker(·, ·) whose (i, j)-entry is defined
as
Ki,j := ker(xi, xj),
∀i, j ∈[L].
Let z := φ(x). Then, the distributionally robust x-nonlinear
estimation problem (57) can be rewritten as a distributionally
robust z-linear beamforming problem as
min
W
max
Rz,Rzs,Rs
Tr

W RzW T −W Rzs −RT
zsW T + Rs

s.t.
d0
  Rz
Rzs
RT
zs
Rs

,
"
ˆRz
ˆRzs
ˆRT
zs
ˆRs
#!
≤ϵ0,
 Rz
Rzs
RT
zs
Rs

⪰0,
(58)
where ˆRz = 1
LK2, ˆRzs = 1
LKST, ˆRs = 1
LSST, and S :=
[Re S; Im S] = [s1, s2, . . . , sL]. In addition, the strong min-
max property holds for (58): i.e., the order of min and max
can be exchanged provided that the first constraint is compact
convex. As a result, given every pair of (Rz, Rzs, Rs), the
optimal Wiener beamformer is
W ⋆
RKHS = RT
zs · R−1
z
(59)
which transforms (58) to
max
Rz,Rzs,Rs
Tr

−RT
zsR−1
z Rzs + Rs

s.t.
d0
  Rz
Rzs
RT
zs
Rs

,
"
ˆRz
ˆRzs
ˆRT
zs
ˆRs
#!
≤ϵ0,
 Rz
Rzs
RT
zs
Rs

⪰0,
Rz ≻0.
(60)
Proof: Treating [z; s] as, or approximating [z; s] using,
a joint Gaussian random vector due to the linear estimation
relation ˆs = W z in RKHS [cf. (57)], then the results in
Lemma 1 apply. For details, see Appendix G.
□
In (58), d0 defines a matrix similarity measure to quantify
the uncertainty of the covariance matrix of [z; s], and ϵ0 ≥0
quantifies the uncertainty level. Proposition 3 reveals the ben-
efit of the kernel trick (2), that is, the possibility to represent
a nonlinear estimation problem as a linear one.
The claim below summarizes the solution of (17) in the
RKHS induced by the kernel function ker(·, ·).
Claim 2: Suppose that (R⋆
z, R⋆
zs, R⋆
s) solves (60). Then the
optimal estimator of (17) in the RKHS induced by ker(·, ·) is
given by
ϕ⋆(x) = ΓM · R⋆T
zs · R⋆−1
z
· φ(x),
(61)
where x = [Re x; Im x] is the real-space representation of x,
ΓM := [IM, JM] is defined in Subsection I-B, and
φ(x) :=


ker(x, x1)
ker(x, x2)
...
ker(x, xL)

.
In addition, the corresponding worst-case estimation error
covariance is
ΓM ·

−R⋆T
zs R⋆−1
z
R⋆
zs + R⋆
s

· ΓH
M,
(62)
which upper bounds the true estimation error covariance.
□
Concrete examples of Claim 2 are given as follows.
Example 4 (Kernelized Diagonal Loading): By using the
trimmed diagonal-loading uncertainty set for Rz, i.e.,
ˆRz −ϵ0IL ⪯Rz ⪯ˆRz + ϵ0IL,
we have the kernelized diagonal loading method
ϕ⋆(x) = ΓM · 1
LSK ·
 1
LK2 + ϵ0IL
−1
· φ(x),
(63)
which is obtained at the upper bound of Rz. Furthermore,
in this case, the distributionally robust formulation (57) is
equivalent to a squared-F-norm-regularized formulation
min
W Tr E(x,s)∼ˆPx,s[W · φ(x) −s][W · φ(x) −s]T+
ϵ0 · Tr[W W T],
(64)
which can be proven by replacing Rz in (58) with its upper
bound.
□

10
Example 5 (Kernelized Eigenvalue Thresholding): The ker-
nelized eigenvalue thresholding method can be designed in
analogy to Example 2. The two key steps are to obtain the
eigenvalue decomposition of ˆRz = K2/L and then lift the
eigenvalues; cf. (40).
□
In addition, Example 4 motivates the following important
theorem for statistical machine learning.
Theorem 3 (Kernel Ridge Regression and Kernel Tikhonov
Regularization): Consider the nonlinear regression problem
s = ϕ(x) + e,
and the distributionally robust estimator of ϕ(x) = W · φ(x)
in the RKHS induced by the kernel function ker(·, ·), i.e.,
min
W ∈R2M×L
max
Px,s∈Ux,s Tr Ex,s[W · φ(x) −s][W · φ(x) −s]T.
Supposing that only the second-order moment of z := φ(x)
is uncertain and quantified as
ˆRz −ϵ0IL ⪯Rz ⪯ˆRz + ϵ0IL,
then the distributionally robust estimator of W becomes a
kernel ridge regression method (64). The regularization term
in (64) becomes the Tikhonov regularizer Tr[W F W T] if
ˆRz −ϵ0F ⪯Rz ⪯ˆRz + ϵ0F
for some F ⪰0.
Proof: See Example 4; cf. Theorem 2.
□
Theorem 3 gives the kernel ridge regression an interpre-
tation of distributional robustness. The usual choice of F in
Theorem 3 is the L-divided kernel matrix K/L; see, e.g., [36,
Eq. (4)], [24, Eqs. (15.110) and (15.113)]. As a result, from
(63), we have
ϕ⋆(x) = ΓM · S · (K + ϵ0IL)−1 · φ(x),
(65)
which is another type of kernel ridge regression (i.e., a new
kernelized diagonal-loading method).
In analogy to Corollary 2, the following corollary motivated
from (64) is immediate.
Corollary
4:
The
following
squared-norm-regularized
method in RKHSs can combat the distributional uncertainty:
min
W Tr E(x,s)∼ˆPx,s[W · φ(x) −s][W · φ(x) −s]T+
λ · ∥W ∥2,
(66)
for any matrix norm ∥· ∥; cf. Corollary 2.
□
Moreover, in analogy to Corollary 3, the following corollary
is immediate.
Corollary 5 (Data Augmentation for Kernel Regression):
Consider the nonlinear regression problem in Theorem 3. Its
data-perturbed counterpart can be constructed by taking into
account the data perturbation vectors (∆s, ∆z). Suppose that
∆z is uncorrelated with z, with s, and with ∆s; in addition,
∆s is uncorrelated with z. If the second-order moment of
∆z is upper bounded by ϵ0IL, then the distributionally robust
estimator of W becomes a kernel ridge regression (i.e.,
squared-F-norm regularized) method (64). The regularization
term becomes Tr

W F W H
, which is known as the Tikhonov
regularizer, if the second-order moment of ∆z is upper
bounded by ϵ0F for some F ⪰0.
□
General uncertainty sets using the Wasserstein distance or
the F-norm, beyond the diagonal ϵ0-perturbation (cf. Example
4), can be straightforwardly employed and the distributional
robustness modeling and analyses remain routine; cf. Sub-
section IV-B. Hence, we omit them here. However, such
complicated approaches are computationally prohibitive in
practice when L or M is large.
2) Multi-Frame Case: Dynamic Channel Evolution: As in
(54), the multi-frame formulation in RKHSs is
min
W ∈R2M×L Tr Ex,s[W · φ(x) −s][W · φ(x) −s]T+
λ · Tr[W −W ′][W −W ′]T,
(67)
where W ′ denotes the beamformer in the immediately pre-
ceding frame and serves as a prior knowledge of W .
Claim 3 (Multi-Frame Estimation in RHKS): The solution
to (67) is given by (cf. (59))
W ⋆
RKHS-MF
= [Rzs + λW ′T]T[Rz + λIL]−1
=
  1
LSK + λW ′
·
  1
LK2 + λIL
−1 ,
(68)
where λ ≥0 is a tuning parameter to control the similarity
between W and W ′; cf. Claim 1.
□
The remaining distributional robustness modeling and anal-
yses on (67) against the uncertainties in ˆRz, ˆRxz, and ˆRs are
technically straightforward; cf. Subsection IV-C. Therefore, we
omit them here.
B. Neural Networks
With the W -parameterization ϕW[R](x) of ϕ(x) in feedfor-
ward multi-layer neural networks, i.e., (4), the distributionally
robust estimation problem (17) becomes
min
W[R]
max
Px,s∈Ux,s Tr Ex,s[ϕW[R](x) −s][ϕW[R](x) −s]T, (69)
where W[R] := {W1, W2, . . . , WR} and ϕW[R](x) is defined
in (4). Problem (69) is highly nonlinear in both argument
x and parameter W[R], which is different from the case in
reproducing kernel Hilbert spaces where the W -linearization
features. Hence, problem (69) is too complicated to solve to
global optimality. According to [27, Cor. 33], under several
technical conditions (plus the boundedness of the feasible
region of W[R]), (69) is upper bounded by a spectral-norm-
regularized empirical risk minimization problem
min
W[R]
1
L
L
X
i=1
Tr[ϕW[R](xi) −si][ϕW[R](xi) −si]T+
λ′ ·
R
X
r=1
∥Wr∥2,
(70)
for some regularization coefficient λ′ ≥0, where ∥·∥2 denotes
the spectral norm of a matrix (i.e., the induced 2-norm).
Eq. (70) rigorously justifies the popular norm regularization
method in training neural networks: By diminishing the upper
bound (70) of (69), the true error in (69) can be controlled from
above. The regularized ERM problem (70) is reminiscent of
the ridge regression and the kernel ridge regression methods
in Theorems 2 and 3 for distributional robustness in linear re-
gression and RKHS linear regression, respectively. Supposing

11
that W ⋆
[R] is an (approximated, or sub-optimal) solution3 of
(70), then the distributionally robust optimal estimator of the
transmitted signal s can be obtained as
ˆs = ΓM · ϕW ⋆
[R](x).
Therefore, in training a neural network for wireless signal
estimation, it is recommended to apply the norm regularization
methods. Since norms on real spaces are equivalent, (70) can
be further upper bounded by
min
W[R]
1
L
L
X
i=1
Tr[ϕW[R](xi) −si][ϕW[R](xi) −si]T+
λ ·
R
X
r=1
∥Wr∥,
(71)
for any matrix norm ∥· ∥and some λ ≥0; λ depends on
λ′ and ∥· ∥. As a result, to achieve distributional robustness
in training a neural network, any-norm-regularized learning
method in (71) can be considered.
VI. EXPERIMENTS
We consider a point-to-point multiple-input-multiple-output
(MIMO) wireless communication problem where the trans-
mitter is located at [0, 0] and the receiver is at [500m, 450m].
We randomly sample 25 points according to the uniform
distribution on the square of [0, 500m] × [0, 500m] to denote
the scatters’ positions; i.e., there exist 26 radio paths. All
the source data and codes are available online at GitHub
with thorough implementation comments: https://github.com/
Spratm-Asleaf/DRRC. In this section, we only present major
experimental setups and results; readers can use the shared
source codes to explore (or verify) minor ones.
The following eleven methods are implemented in the
experiments: 1) Wiener: Wiener beamformer (12), upper
expression; 2) Wiener-DL: Wiener beamformer with diagonal
loading (30), upper expression; 3) Wiener-DR: Distribution-
ally robust Wiener beamformer (49) and (53); 4) Wiener-CE:
Channel-estimation-based Wiener beamformer (12), lower
expression;
5)
Wiener-CE-DL:
Channel-estimation-based
Wiener beamformer with diagonal loading (30), lower ex-
pression; 6) Wiener-CE-DR: Distributionally robust channel-
estimation-based Wiener beamformer (42) and (31); 7) Capon:
Capon beamformer (39) for ϵ0 = 0; 8) Capon-DL: Capon
beamformer with diagonal loading (39); 9) ZF: Zero-forcing
beamformer where WZF := ( ˆ
HH ˆ
H)−1 ˆ
HH and ˆ
H denotes
the estimated channel matrix; 10) Kernel: Kernel receiver (61)
with ϵ0 = 0 in (60); and 11) Kernel-DL: Kernel receiver
with diagonal loading (65). Note that the diagonal-loading-
based methods are particular cases of distributionally robust
combiners; see, e.g., Corollary 1 and Example 4. The deep-
learning-based (DL-based) methods in Subsection V-B are not
implemented in this section because they have been deeply
studied in our previous publications, e.g., [10], [12]; we only
comment on the advantages and disadvantages of DL-based
methods compared with the listed eleven methods in Section
VII (Conclusions).
3Neural networks are hard to be globally optimized.
When covariance matrix Rs of transmitted signal s is
unknown for the receiver (e.g., in ISAC systems, Rs needs to
vary from one frame to another for sensing), Rs is estimated
by the sample covariance matrix ˆRs = SSH/L. The channel
matrix H is estimated using the minimum mean-squared error
method, i.e., ˆ
H = XSH(SSH)−1. Covariance matrix Rv of
channel noise v is estimated using the least-square method,
i.e., ˆRv = (X −ˆ
HS)(X −ˆ
HS)H/L. The matrices ˆRs,
ˆ
H, and ˆRv are therefore uncertain compared to their true
(but unknown; possibly time-varying) values Rs, H, and
Rv, respectively. The matrices
ˆRs,
ˆ
H, and
ˆRv are used
in beamformers such as the channel-estimation-based Wiener
beamformer (30), the Capon beamformer, and the zero-forcing
beamformer.
The combiners are determined on the training data set
(i.e., pilot data). The performance evaluation method of com-
biners is mean-squared estimation error (MSE) on the test
data set (i.e., non-pilot communication data): to be specific,
∥Stest −ˆStest∥2
F /(M × Ltest) where Stest ∈CM×Ltest is the test
data block, ˆStest is its estimate, and Ltest is the length of non-
pilot test data units. As data-driven machine learning methods,
all parameters (e.g., uncertainty quantification coefficients ϵ’s)
of combiners can be tuned using the popular cross-validation
(e.g., one-shot cross-validation) method. The parameters can
also be empirically tuned to save training times because cross-
validation imposes a significant computational burden. This
article mainly uses the empirical tuning method (i.e., trial-
and-error) to tune each combiner to achieve its best average
performance. For each test case, the MSE performances are
averaged on 250 Monte–Carlo episodes.
We consider an experimental scenario where impulse chan-
nel noises exist; i.e., the channel is non-Gaussian so linear
beamformers are no longer sufficient. (Complementary exper-
imental setups and results can be seen in online supplementary
materials.) The detailed setups are as follows. The transmitter
has four antennas (i.e., M = 4) with unit transmit power;
without loss of generality, each antenna is assumed to emit
continuous-valued complex Gaussian signals. The receiver has
eight antennas (i.e., N = 8). The SNR is −10dB, which
is a challenging situation. The channel has impulse noises:
i.e., in L received signals (i.e., [x1, x2, . . . , xL]) that are
contaminated by usual complex Gaussian channel noises, 10%
of them are also contaminated by uniform noises with the
maximum amplitude of 1.5, which is a relatively large value
compared to the amplitude of the usual Gaussian channel
noises. We assume that a communication frame contains 500
non-pilot data units; i.e., Ltest = 500. The experimental results
are shown in Tables I∼VI, from which the following main
points can be outlined.
1) A larger number of pilot data benefits the estimation
performances of wireless signals.
2) The diagonal loading operation can significantly improve
the estimation performances especially when the pilot
data size is relatively small.
3) Since the signal model under impulse channel noises
is no longer linear Gaussian, the optimal combiner in
the MSE sense must be nonlinear. Therefore, the Kernel
and the Kernel-DL methods have the potential to outper-

12
form other linear beamformers, i.e., to suppress outliers.
However, in practice, the non-robust Kernel method may
undergo numerical instability in calculating the inverse
of the kernel matrix K. Therefore, its actual MSEs are
not necessarily smaller than those of linear beamformers.
Nevertheless, the robust Kernel-DL method consistently
outperforms all other beamformers.
4) Distributionally robust combiners (including diagonal-
loading ones) can combat the adverse effect introduced
by the limited pilot size and several types of uncertainties
in the signal model (e.g., outliers). To be specific, for
example, all diagonal-loading combiners can outperform
their original non-diagonal-loading counterparts; cf. the
Wiener and the Wiener-DL methods, the Wiener-CE and
the Wiener-CE-DL methods, the Capon and the Capon-
DL methods, and the Kernel and the Kernel-DL methods.
In addition, the Wiener-DR beamformer (53) using the
F-norm uncertainty set has the potential to outperform
the Wiener-DL beamformer (30) that employs the simple
uncertainty set (26).
5) Although the Wiener-DR beamformer has the potential
to work better than the Wiener-DL beamformer, it has
a significant computational burden, which may not be
suitable for timely use in practice especially when the
computing resources are limited. Hence, the Wiener-
DL beamformer is practically promising because it can
provide an excellent balance between the computational
burden and the actual performance.
Remarks on Parameter Tuning: From experiments, we find
that the uncertainty quantification coefficients ϵ’s (e.g., in diag-
onal loading) can be neither too large nor too small. When ϵ’s
are too large, the combiners become overly conservative, while
when ϵ’s are too small, the combiners cannot offer sufficient
robustness against data scarcity and model uncertainties. In
both cases of inappropriate ϵ’s, the performances of combiners
degrade significantly. Therefore, ϵ’s must be carefully tuned
in practice, and a rigorous method to tune ϵ’s can be the
cross-validation method on the training data set (i.e., the pilot
data set). If practitioners just pursue satisfaction rather than
optimality, the empirical tuning method is recommended to
save training times.
TABLE I
EXPERIMENTAL RESULTS (PILOT SIZE = 10)
Combiner
MSE
Time
Combiner
MSE
Time
Wnr
3.30
1.49e-04
Wnr-DL
2.11
9.81e-06
Wnr-DR
1.97
3.16e+00
Wnr-CE
3.30
4.59e-05
Wnr-CE-DL
2.50
2.17e-05
Wnr-CE-DR
3.31
4.63e-05
Capon
5.44
4.42e-05
Capon-DL
4.52
2.50e-05
ZF
2.12
2.54e-05
Kernel
1.07
1.60e-04
Kernel-DL
0.80
5.59e-05
Wnr: The abbreviation for Wiener.
Time: The training time averaged on 250 Monte–Carlo episodes.
VII. CONCLUSIONS
This article introduces a unified mathematical framework
for receive combining of wireless signals from the perspective
TABLE II
EXPERIMENTAL RESULTS (PILOT SIZE = 15)
Combiner
MSE
Time
Combiner
MSE
Time
Wnr
1.38
1.65e-04
Wnr-DL
1.23
1.10e-05
Wnr-DR
1.07
3.21e+00
Wnr-CE
1.38
4.44e-05
Wnr-CE-DL
1.30
2.12e-05
Wnr-CE-DR
1.39
4.28e-05
Capon
4.48
4.31e-05
Capon-DL
4.34
2.42e-05
ZF
2.97
2.44e-05
Kernel
1.12
1.94e-04
Kernel-DL
0.70
9.23e-05
TABLE III
EXPERIMENTAL RESULTS (PILOT SIZE = 20)
Combiner
MSE
Time
Combiner
MSE
Time
Wnr
1.12
1.86e-04
Wnr-DL
1.05
1.87e-05
Wnr-DR
0.93
7.19e+00
Wnr-CE
1.12
5.78e-05
Wnr-CE-DL
1.08
3.14e-05
Wnr-CE-DR
1.13
6.01e-05
Capon
5.01
5.93e-05
Capon-DL
4.94
3.81e-05
ZF
3.82
3.56e-05
Kernel
1.20
4.48e-04
Kernel-DL
0.66
3.11e-04
TABLE IV
EXPERIMENTAL RESULTS (PILOT SIZE = 25)
Combiner
MSE
Time
Combiner
MSE
Time
Wnr
0.92
1.41e-04
Wnr-DL
0.88
1.11e-05
Wnr-DR
0.80
4.22e+00
Wnr-CE
0.92
5.02e-05
Wnr-CE-DL
0.90
2.44e-05
Wnr-CE-DR
0.92
4.78e-05
Capon
4.94
4.93e-05
Capon-DL
4.89
2.85e-05
ZF
4.06
2.72e-05
Kernel
1.14
4.26e-04
Kernel-DL
0.60
2.95e-04
TABLE V
EXPERIMENTAL RESULTS (PILOT SIZE = 50)
Combiner
MSE
Time
Combiner
MSE
Time
Wnr
0.69
1.75e-04
Wnr-DL
0.68
1.85e-05
Wnr-DR
0.65
6.10e+00
Wnr-CE
0.69
5.81e-05
Wnr-CE-DL
0.68
3.03e-05
Wnr-CE-DR
0.70
5.90e-05
Capon
6.95
5.97e-05
Capon-DL
6.93
3.75e-05
ZF
6.36
3.38e-05
Kernel
0.92
1.81e-03
Kernel-DL
0.53
1.67e-03
TABLE VI
EXPERIMENTAL RESULTS (PILOT SIZE = 100)
Combiner
MSE
Time
Combiner
MSE
Time
Wnr
0.57
3.41e-04
Wnr-DL
0.57
3.64e-05
Wnr-DR
0.55
4.96e+00
Wnr-CE
0.57
6.35e-05
Wnr-CE-DL
0.57
2.93e-05
Wnr-CE-DR
0.58
6.07e-05
Capon
9.89
6.88e-05
Capon-DL
9.88
3.99e-05
ZF
9.45
3.27e-05
Kernel
0.72
5.93e-03
Kernel-DL
0.49
5.83e-03

13
of data-driven machine learning, which reveals that channel
estimation is not a necessary operation. To combat the limited
pilot size and several types of uncertainties in the signal model,
the distributionally robust (DR) receive combining framework
is then suggested. We prove that the diagonal-loading (DL)
methods are distributionally robust against the scarcity of pilot
data and the uncertainties in the signal model. In addition,
we generalize the diagonal-loading methods to achieve better
estimation performance (e.g., the DR Wiener beamformer
using F-norm for uncertainty quantification), at the cost of sig-
nificantly higher computational burdens. Experiments suggest
that nonlinear combiners such as the Kernel and the Kernel-DL
methods have the potential when the pilot size is small and/or
the signal model is not linear Gaussian. Compared with the
Kernel and the Kernel-DL combiners, neural-network-based
solutions [10], [12] have a stronger expressive capability of
nonlinearities, which however are unscalable in the numbers
of transmit and receive antennas, and significantly more time-
consuming in training and more troublesome in tuning hyper-
parameters (e.g., the number of layers and the number of
neurons in each layer) than the studied eleven combiners.
APPENDIX A
STRUCTURED REPRESENTATION OF NONLINEAR
FUNCTIONS
In Section II, we have reviewed two popular frameworks for
representing (nonlinear) functions: reproducing kernel Hilbert
spaces (RKHS) and neural network function spaces (NNFS).
Typical kernel functions ker(·, ·) to define RKHSs include
Gaussian kernel, Matern kernel, Linear kernel, Laplacian
kernel, and Polynomial kernel. Mathematical details of these
kernel functions can be found in [24, Subsec. 14.2], [27,
Ex. 1]. Typical activation functions σ(·) to define NNFSs
include Hyperbolic tangent (i.e, tanh) function, Softmax func-
tion, Sigmoid function, Rectified linear unit (ReLU) function,
and Exponential linear unit (ELU) function. Mathematical
details of these activation functions can be found in [27, Ex. 2].
APPENDIX B
DETAILS ON REAL-SPACE SIGNAL REPRESENTATION
Let Rx := ExxH, Cx := ExxT, Cs := EssT, and Cv :=
EvvT = 0. We have
Rx := ExxT = 1
2

Re(Rx + Cx)
Im(−Rx + Cx)
Im(Rx + Cx)
Re(Rx −Cx)

.
Rs := EssT
= 1
2
 Re(Rs + Cs)
Im(−Rs + Cs)
Im(Rs + Cs)
Re(Rs −Cs)

,
and
Rv := EvvT
= 1
2

Re Rv
Im −Rv
Im Rv
Re Rv

.
Note that the following identities hold: Rx = HRsHH+Rv,
Cx = HCsHT, Rx = H·Rs·HT+Rv, and Rxs = H·Rs.
APPENDIX C
EXTENSIVE READING ON DISTRIBUTIONAL UNCERTAINTY
A. Generalization Error and Distributional Robustness
We use (7) and (15) as examples to illustrate the concepts.
Supposing that ϕ⋆solves the true problem (7) and ϕ⋆
ERM solves
the surrogate problem (15), we have
min
ϕ Tr E(x,s)∼Px,s[ϕ(x) −s][ϕ(x) −s]H
= Tr E(x,s)∼Px,s[ϕ⋆(x) −s][ϕ⋆(x) −s]H
≤Tr E(x,s)∼Px,s[ϕ⋆
ERM(x) −s][ϕ⋆
ERM(x) −s]H.
(72)
To clarify further, the testing error in the last line (evaluated at
the true distribution Px,s) of the learned estimator ϕ⋆
ERM may
be (much) larger than the optimal error in the first two lines,
although ϕ⋆
ERM has the smallest training error (evaluated at the
nominal distribution ˆPx,s), i.e.,
min
ϕ Tr E(x,s)∼ˆPx,s[ϕ(x) −s][ϕ(x) −s]H
= minϕ Tr 1
L
PL
i=1[ϕ(xi) −si][ϕ(xi) −si]H
= Tr 1
L
PL
i=1[ϕ⋆
ERM(xi) −si][ϕ⋆
ERM(xi) −si]H
≤Tr 1
L
PL
i=1[ϕ⋆(xi) −si][ϕ⋆(xi) −si]H.
(73)
In the terminologies of machine learning, the difference be-
tween the testing error and the training error, i.e.,
Tr E(x,s)∼Px,s[ϕ⋆
ERM(x) −s][ϕ⋆
ERM(x) −s]H−
Tr E(x,s)∼ˆPx,s[ϕ⋆
ERM(x) −s][ϕ⋆
ERM(x) −s]H
= Tr Ex,s[ϕ⋆
ERM(x) −s][ϕ⋆
ERM(x) −s]H−
Tr 1
L
L
X
i=1
[ϕ⋆
ERM(xi) −si][ϕ⋆
ERM(xi) −si]H
is called the generalization error of ϕ⋆
ERM; the difference
between the testing error and the optimal error, i.e.,
Tr Ex,s[ϕ⋆
ERM(x) −s][ϕ⋆
ERM(x) −s]H−
Tr Ex,s[ϕ⋆(x) −s][ϕ⋆(x) −s]H
is called the excess risk of ϕ⋆
ERM. In machine learning practice,
we want to reduce both the generalization error and the excess
risk. Most attention in the literature has been particularly paid
to reducing generalization errors. Specifically, an upper bound
of the true cost Tr E(x,s)∼Px,s[ϕ(x) −s][ϕ(x) −s]H is first
found and then minimize the upper bound: by minimizing the
upper bound, the true cost can also be reduced.
Fact 1: Suppose that the true distribution P0,x,s of (x, s) is
included in Ux,s; for notational clarity, we hereafter distinguish
P0,x,s from Px,s. The true objective function evaluated at
P0,x,s, i.e.,
Tr E(x,s)∼P0,x,s[ϕ(x) −s][ϕ(x) −s]H,
∀ϕ ∈B,
(74)
is upper bounded by the worst-case objective function of (17),
i.e.,
max
Px,s∈Ux,s Tr E(x,s)∼Px,s[ϕ(x)−s][ϕ(x)−s]H,
∀ϕ ∈B. (75)
Therefore, by diminishing the upper bound in (75), the true
estimation error evaluated at P0,x,s can also be reduced.
However, the conventional empirical estimation error evaluated
at ˆPx,s cannot upper bound the true estimation error (74).

14
This performance guarantee is the benefit of considering
the distributionally robust method (17). Due to the weak
convergence property of the empirical distribution to the true
data-generating distribution, that is, d(P0,x,s, ˆPx,s) →0 as
the sample size L →∞, there exists ϵ in (18) for every L,
such that P0,x,s is included in Ux,s in PL
0,x,s-probability (L-
fold product measure of P0,x,s).
□
B. Non-Stationary Channel Statistics
In the main body of the article (see also Fact 1), we
assume that the true data-generating distribution P0,x,s is time-
invariant within a frame. In real-world operations, however,
this assumption might be untenable.
As shown in Fig. 1, the frame contains eight data units; we
suppose that the first four units are pilot symbols and the rest
four units are communication-data symbols.
Time
A Frame
t0t0
t2t2
t4t4
t6t6
t8t8
Fig. 1. True data-generating distributions might be time-varying in a frame.
Let P0,x,s,i denote the true data-generating distribution at
time point ti where i = 1, 2, . . . , 8. Specifically, we have
(xi, si) ∼P0,x,s,i for every i. Therefore, the pilot data set (i.e.,
the training data set) {(x1, s1), (x2, s2), (x3, s3), (x4, s4)}
can
be
seen
as
realizations
of
the
mean
distribution
Ptrain,0,x,s of underlying true training-data distributions where
Ptrain,0,x,s = P4
i=1 hiP0,x,s,i, which is a mixture distribution
with mixing weights 0 ≤h1, h2, h3, h4 ≤1; P4
i=1 hi = 1.
Similarly, the communication data set (i.e., the testing data set)
{(x5, s5), (x6, s6), (x7, s7), (x8, s8)} can be seen as realiza-
tions of the mean Ptest,0,x,s of the underlying true testing-data
distributions where Ptest,0,x,s = P8
i=5 hiP0,x,s,i, with mixing
weights 0 ≤h5, h6, h7, h8 ≤1; P8
i=5 hi = 1.
Suppose that
d(ˆPtrain,x,s, Ptrain,0,x,s) ≤ϵ1,
where ˆPtrain,x,s := 1
4
P4
i=1 δ(xi,si) is the data-driven estimate
of Ptrain,0,x,s and
d(Ptrain,0,x,s, Ptest,0,x,s) ≤ϵ2,
for some ϵ1, ϵ2 ≥0. We have the uncertainty quantification
d(Ptest,0,x,s, ˆPtrain,x,s) ≤ϵ := ϵ1 + ϵ2.
Therefore, the distributionally robust modeling and solution
framework is still valid to hedge against the distributional
uncertainty in the nominal distribution ˆPtrain,x,s compared to
the underlying true distribution Ptest,0,x,s. When Ptrain,0,x,s =
Ptest,0,x,s, as assumed in the main body of the article, we have
ϵ1 →0 and ϵ →ϵ2 = 0 as the pilot size tends to infinity;
however, when Ptrain,0,x,s ̸= Ptest,0,x,s, the radius ϵ →ϵ2 ̸= 0
although ϵ1 →0.
Another justification for the DRO method is as follows.
Suppose that there exists ϵ ≥0 such that
d(P0,x,s,i, ˆPtrain,x,s) ≤ϵ,
∀i ∈{1, 2, . . . , 8}.
It means that, at every snapshot in the frame, the true data-
generating distribution is included in the uncertainty set.
Hence, the DRO cost can still upper bound the true cost even
though the true distribution is time-varying; cf. Fact 1.
APPENDIX D
ADDITIONAL DISCUSSIONS ON DISTRIBUTIONALLY
ROBUST ESTIMATION
To develop this article, the typical minimum mean-squared
error (MSE) criterion is employed; see (7) and (10). Accord-
ingly, the distributionally robust receive combining framework
in this article is exemplified using the MSE cost function. The
cost function for wireless signal estimation, however, can be
any Borel-measurable function h : CM × CM →R+. As a
result, the optimal estimation problem under the distribution
Px,s is given by
min
ϕ∈BCN →CM Ex,sh[ϕ(x), s].
(76)
Specific examples of h in wireless communications can be,
e.g., mean absolute error, Huber’s cost function [37], [38]
where h is no longer quadratic as in (7) and (10). Accord-
ingly, when the distributional uncertainty exists in Px,s, the
distributionally robust receive combining framework becomes
min
ϕ∈BCN →CM
max
Px,s∈Ux,s Ex,sh[ϕ(x), s].
(77)
Problem (77) is generally challenging to solve because it is an
infinite-dimensional program. Therefore, in practice, we can
limit the feasible region of ϕ to a parameterized subspace of
BCN→CM , for example, a reproducing kernel Hilbert space
H or a neural network function space K; see Section II.
Consequently, Problem (77) is approximated by the following
finite-dimensional (in terms of W ) program
min
W
max
Px,s∈Ux,s Ex,sh[ϕW (x), s],
(78)
where W parameterizes ϕ and lies in real or complex coordi-
nate spaces; note that both H and K can be dense in B. Under
the MSE cost function, (78) is particularized in (19) for linear
function spaces, in (57) for reproducing kernel Hilbert spaces,
and in (69) for neural network function spaces, which build
this article in a technically tractable manner.
The distributionally robust receive combining problem (77)
under generic cost functions h and generic feasible regions
of ϕ can be technically challenging. Even for the simplified
problem (78), the solution method can be quite complex, and
closed-form solutions cannot be generally guaranteed; see,
e.g., [39], [40]. The complication further arises when the
distributional uncertainty sets Ux,s for Px,s are complicated;
see, e.g., [32]. Therefore, this article serves as the starting
point of distributionally robust receive combining, in which
closed-form solutions are largely ensured by leveraging
F1) the MSE cost function as in (7) and (10);
F2) the linear function spaces as in (19) and reproducing
kernel Hilbert spaces as in (57);
F3) the second-moment-based uncertainty sets in Defini-
tions 1, 2, 3, and 4; see also Corollary 1, Claim 2, and
Example 4.

15
Note that even under the features F1) and F2), the closed-
form solutions cannot be guaranteed. For example, if Wasser-
stein or F-norm uncertainty sets are used, the associated
distributionally robust receive combining problems can be
computationally heavy; see (47) and (52) as well as Proposi-
tions 1 and 2. However, for emerging high-performance com-
puting devices, the computational burden may be no longer
an issue in the future. Hence, advanced distributionally robust
receive combining formulations based on (77) and (78) are
still attractive for future-generation communication systems.
This article seeks to provide a foundation for this direction.
APPENDIX E
PROOF OF LEMMA 1
Proof: The objective function of Problem (19) equals to

W HW
−W H
−W
IM

,
 Rx
Rxs
RH
xs
Rs

,
(79)
where ⟨A, B⟩:= Tr AHB for two matrices A and B.
Therefore, the objective function of (19) is convex in W and
linear (thus concave) in the matrix variable R. Hence, due to
Sion’s minimax theorem [41, Corollary 3.3], Problem (19) is
equivalent to
max
R min
W
Tr

W RxW H −W Rxs −RH
xsW H + Rs

s.t.
d0(R,
ˆR) ≤ϵ0,
R ⪰0.
(80)
Note that the feasible region of R is compact convex, and that
of W (i.e., CM×N) is convex.
For every given R, the inner minimization sub-problem of
(80) is solved by the Wiener beamformer W ⋆
Wiener = RH
xsR−1
x ,
which transforms (80) to (21). This completes the proof.
□
APPENDIX F
PROOF OF THEOREM 1
Proof: Consider the following optimization problem
max
R
Tr

−RH
xsR−1
x Rxs + Rs

s.t.
R ⪰R2,
Rx ≻0,
(81)
which, due to Lemma 1, is equivalent [in the sense of the same
optimal objective value and maximizer(s) R⋆] to
min
W max
R

W HW
−W H
−W
IM

,
 Rx
Rxs
RH
xs
Rs

s.t.
R ⪰R2,
Rx ≻0.
(82)
Note that

W HW
−W H
−W
IM

⪰0, because for all x ∈CN
and y ∈CM, we have
[xH, yH]

W HW
−W H
−W
IM
  x
y

= ∥W x −y∥2
2 ≥0.
Therefore, for every given W , the objective function of (82)
is increasing in R. As a result, the objective value of (81) is
lower-bounded at R2: To be specific, ∀R ⪰R2, we have
Tr

−RH
xsR−1
x Rxs +Rs

≥Tr

−RH
2,xsR−1
2,xR2,xs +R2,s

,
i.e., f1(R) ≥f1(R2), which proves the first part.
On the other hand, if R1,x
⪰R2,x
≻0, we have
R−1
2,x
⪰
R−1
1,x. As a result, f2(R1,x) −f2(R2,x)
=
Tr

RH
xs(R−1
2,x −R−1
1,x)Rxs

≥0, completing the proof.
□
APPENDIX G
PROOF OF PROPOSITION 3
Proof: Letting z := φ(x), (57) can be rewritten as
min
W ∈R2M×L
max
Pz,s∈Uz,s Tr Ez,s[W z −s][W z −s]T.
(83)
Tantamount to the distributionally robust beamforming prob-
lem (19), Problem (83) reduces to (58) where
ˆRz := 1
L
L
X
i=1
zizT
i = 1
L
L
X
i=1
φ(xi)φT(xi) = 1
LK2,
ˆRzs := 1
L
L
X
i=1
zisT
i = 1
L
L
X
i=1
φ(xi) · sT
i = 1
LKST,
ˆRs := 1
L
L
X
i=1
sisT
i = 1
L
L
X
i=1
si · sT
i = 1
LSST,
and
K := [φ(x1), φ(x2), . . . , φ(xL)] ∈RL×L.
The rest claims are due to Lemma 1; NB: K is invertible. □
REFERENCES
[1] T. Lo, H. Leung, and J. Litva, “Nonlinear beamforming,” Electronics
Letters, vol. 4, no. 27, pp. 350–352, 1991.
[2] S. Yang and L. Hanzo, “Fifty years of MIMO detection: The road to
large-scale MIMOs,” IEEE Commun. Surveys Tuts., vol. 17, no. 4, pp.
1941–1988, 2015.
[3] A. M. Elbir, K. V. Mishra, S. A. Vorobyov, and R. W. Heath, “Twenty-
five years of advances in beamforming: From convex and nonconvex
optimization to learning techniques,” IEEE Signal Processing Mag.,
vol. 40, no. 4, pp. 118–131, 2023.
[4] S. Chen, S. Tan, L. Xu, and L. Hanzo, “Adaptive minimum error-rate
filtering design: A review,” Signal Processing, vol. 88, no. 7, pp. 1671–
1697, 2008.
[5] S. Chen, A. Wolfgang, C. J. Harris, and L. Hanzo, “Symmetric RBF
classifier for nonlinear detection in multiple-antenna-aided systems,”
IEEE Trans. Neural Networks, vol. 19, no. 5, pp. 737–745, 2008.
[6] A. Navia-Vazquez, M. Martinez-Ramon, L. E. Garcia-Munoz, and C. G.
Christodoulou, “Approximate kernel orthogonalization for antenna array
processing,” IEEE Trans. Antennas Propagat., vol. 58, no. 12, pp. 3942–
3950, 2010.
[7] M. Neinavaie, M. Derakhtian, and S. A. Vorobyov, “Lossless dimension
reduction for integer least squares with application to sphere decoding,”
IEEE Trans. Signal Processing, vol. 68, pp. 6547–6561, 2020.
[8] J. Liao, J. Zhao, F. Gao, and G. Y. Li, “Deep learning aided low complex
breadth-first tree search for MIMO detection,” IEEE Trans. Wireless
Commun., 2023.
[9] D. A. Awan, R. L. Cavalcante, M. Yukawa, and S. Stanczak, “Robust
online multiuser detection: A hybrid model-data driven approach,” IEEE
Trans. Signal Processing, 2023.
[10] H. Ye, G. Y. Li, and B.-H. Juang, “Power of deep learning for channel
estimation and signal detection in OFDM systems,” IEEE Wireless
Commun. Lett., vol. 7, no. 1, pp. 114–117, 2017.
[11] H. He, C.-K. Wen, S. Jin, and G. Y. Li, “Model-driven deep learning for
MIMO detection,” IEEE Trans. Signal Processing, vol. 68, pp. 1702–
1715, 2020.
[12] N. Van Huynh and G. Y. Li, “Transfer learning for signal detection in
wireless networks,” IEEE Wireless Commun. Lett., vol. 11, no. 11, pp.
2325–2329, 2022.

16
[13] J. Li, P. Stoica, and Z. Wang, “On robust Capon beamforming and
diagonal loading,” IEEE Trans. Signal Processing, vol. 51, no. 7, pp.
1702–1715, 2003.
[14] R. G. Lorenz and S. P. Boyd, “Robust minimum variance beamforming,”
IEEE Trans. Signal Processing, vol. 53, no. 5, pp. 1684–1696, 2005.
[15] X. Zhang, Y. Li, N. Ge, and J. Lu, “Robust minimum variance beam-
forming under distributional uncertainty,” in 2015 IEEE International
Conference on Acoustics, Speech and Signal Processing (ICASSP).
IEEE, 2015, pp. 2514–2518.
[16] B. Li, Y. Rong, J. Sun, and K. L. Teo, “A distributionally robust
minimum variance beamformer design,” IEEE Signal Processing Lett.,
vol. 25, no. 1, pp. 105–109, 2017.
[17] Y. Huang, W. Yang, and S. A. Vorobyov, “Robust adaptive beamforming
maximizing the worst-case SINR over distributional uncertainty sets
for random inc matrix and signal steering vector,” in ICASSP 2022-
2022 IEEE International Conference on Acoustics, Speech and Signal
Processing (ICASSP).
IEEE, 2022, pp. 4918–4922.
[18] Y. Huang, H. Fu, S. A. Vorobyov, and Z.-Q. Luo, “Robust adaptive
beamforming via worst-case SINR maximization with nonconvex un-
certainty sets,” IEEE Trans. Signal Processing, vol. 71, pp. 218–232,
2023.
[19] H. Cox, R. Zeskind, and M. Owen, “Robust adaptive beamforming,”
IEEE Trans. Acoust., Speech, Signal Processing, vol. 35, no. 10, pp.
1365–1376, 1987.
[20] K. Harmanci, J. Tabrikian, and J. L. Krolik, “Relationships between
adaptive minimum variance beamforming and optimal source localiza-
tion,” IEEE Trans. Signal Processing, vol. 48, no. 1, pp. 1–12, 2000.
[21] F. Liu, L. Zhou, C. Masouros, A. Li, W. Luo, and A. Petropulu, “To-
ward dual-functional radar-communication systems: Optimal waveform
design,” IEEE Trans. Signal Processing, vol. 66, no. 16, pp. 4264–4279,
2018.
[22] J. A. Zhang, F. Liu, C. Masouros, R. W. Heath, Z. Feng, L. Zheng,
and A. Petropulu, “An overview of signal processing techniques for
joint communication and radar sensing,” IEEE J. Select. Topics Signal
Processing, vol. 15, no. 6, pp. 1295–1315, 2021.
[23] Y. Xiong, F. Liu, Y. Cui, W. Yuan, T. X. Han, and G. Caire, “On the
fundamental tradeoff of integrated sensing and communications under
Gaussian channels,” IEEE Trans. Inform. Theory, 2023.
[24] K. P. Murphy, Machine Learning: A Probabilistic Perspective.
MIT
Press, 2012.
[25] C. M. Bishop and N. M. Nasrabadi, Pattern Recognition and Machine
Learning.
Springer, 2006, vol. 4, no. 4.
[26] G. Li and J. Ding, “Towards understanding variation-constrained deep
neural networks,” IEEE Trans. Signal Processing, vol. 71, pp. 631–640,
2023.
[27] S. Shafieezadeh-Abadeh, D. Kuhn, and P. M. Esfahani, “Regularization
via mass transportation,” Journal of Machine Learning Research, vol. 20,
no. 103, pp. 1–68, 2019.
[28] M. Staib and S. Jegelka, “Distributionally robust optimization and
generalization in kernel methods,” Advances in Neural Information
Processing Systems, vol. 32, 2019.
[29] E. Delage and Y. Ye, “Distributionally robust optimization under mo-
ment uncertainty with application to data-driven problems,” Operations
Research, vol. 58, no. 3, pp. 595–612, 2010.
[30] S. Wang, “Distributionally robust state estimation for jump linear
systems,” IEEE Trans. Signal Processing, 2023.
[31] J. Li, S. Lin, J. Blanchet, and V. A. Nguyen, “Tikhonov regularization
is optimal transport robust under martingale constraints,” Advances in
Neural Information Processing Systems, vol. 35, pp. 17 677–17 689,
2022.
[32] D. Kuhn, P. M. Esfahani, V. A. Nguyen, and S. Shafieezadeh-Abadeh,
“Wasserstein distributionally robust optimization: Theory and applica-
tions in machine learning,” in Operations Research & Management
Science in the Age of Analytics.
Informs, 2019, pp. 130–166.
[33] J. Blanchet, Y. Kang, and K. Murthy, “Robust Wasserstein profile
inference and applications to machine learning,” Journal of Applied
Probability, vol. 56, no. 3, pp. 830–857, 2019.
[34] C. Shorten and T. M. Khoshgoftaar, “A survey on image data augmen-
tation for deep learning,” Journal of Big Data, vol. 6, 2019.
[35] G. Saon, Z. T¨uske, K. Audhkhasi, and B. Kingsbury, “Sequence noise
injected training for end-to-end speech recognition,” in ICASSP 2019-
2019 IEEE International Conference on Acoustics, Speech and Signal
Processing (ICASSP).
IEEE, 2019, pp. 6261–6265.
[36] K. Vu, J. C. Snyder, L. Li, M. Rupp, B. F. Chen, T. Khelif, K.-R.
M¨uller, and K. Burke, “Understanding kernel ridge regression: Common
behaviors from simple functions to density functionals,” International
Journal of Quantum Chemistry, vol. 115, no. 16, pp. 1115–1128, 2015.
[37] X. Wang and H. V. Poor, “Robust adaptive array for wireless com-
munications,” IEEE J. Select. Areas in Commun., vol. 16, no. 8, pp.
1352–1366, 1998.
[38] V. Katkovnik, M.-S. Lee, and Y.-H. Kim, “Performance study of the
minimax robust phased array for wireless communications,” IEEE Trans.
Wireless Commun., vol. 54, no. 4, pp. 608–613, 2006.
[39] H. Rahimian and S. Mehrotra, “Frameworks and results in distribution-
ally robust optimization,” Open Journal of Mathematical Optimization,
vol. 3, pp. 1–85, 2022.
[40] D. Kuhn, S. Shafiee, and W. Wiesemann, “Distributionally robust
optimization,” Acta Numerica, 2024.
[41] M. Sion, “On general minimax theorems.” Pacific Journal of Mathemat-
ics, vol. 8, no. 1, pp. 171 – 176, 1958.
Shixiong
Wang
(Member, IEEE) received the
B.Eng. degree in detection, guidance, and control
technology, and the M.Eng. degree in systems and
control engineering from the School of Electronics
and Information, Northwestern Polytechnical Uni-
versity, China, in 2016 and 2018, respectively. He
received his Ph.D. degree from the Department of
Industrial Systems Engineering and Management,
National University of Singapore, Singapore, in
2022.
He is currently a Postdoctoral Research Associate
with the Intelligent Transmission and Processing Laboratory, Imperial College
London, London, United Kingdom, from May 2023. He was a Postdoctoral
Research Fellow with the Institute of Data Science, National University of
Singapore, Singapore, from March 2022 to March 2023.
His research interest includes statistics and optimization theories with ap-
plications in signal processing (especially optimal estimation theory), machine
learning (especially generalization error theory), and control technology.
Wei Dai (Member, IEEE) received the Ph.D. degree
from the University of Colorado Boulder, Boulder,
Colorado, in 2007. He is currently a Senior Lecturer
(Associate Professor) in the Department of Electrical
and Electronic Engineering, Imperial College Lon-
don, London, UK. From 2007 to 2011, he was a
Postdoctoral Research Associate with the Univer-
sity of Illinois Urbana-Champaign, Champaign, IL,
USA. His research interests include electromagnetic
sensing, biomedical imaging, wireless communica-
tions, and information theory.
Geoffrey Ye Li is currently a Chair Professor at Im-
perial College London, UK. Before joining Imperial
in 2020, he was a Professor at Georgia Institute of
Technology for 20 years and a Principal Technical
Staff Member with AT&T Labs – Research (previ-
ous Bell Labs) for five years. He made fundamen-
tal contributions to orthogonal frequency division
multiplexing (OFDM) for wireless communications,
established a framework on resource cooperation in
wireless networks, and introduced deep learning to
communications. In these areas, he has published
over 700 journal and conference papers in addition to over 40 granted patents.
His publications have been cited around 80,000 times with an H-index over
130. He has been listed as a Highly Cited Researcher by Clarivate/Web of
Science almost every year.
Dr. Geoffrey Ye Li was elected to Fellow of the Royal Academic of
Engineering (FREng), IEEE Fellow, and IET Fellow for his contributions to
signal processing for wireless communications. He received 2024 IEEE Eric E.
Sumner Award, 2019 IEEE ComSoc Edwin Howard Armstrong Achievement
Award, and several other awards from IEEE Signal Processing, Vehicular
Technology, and Communications Societies.

1
Supplementary Materials
APPENDIX H
ADDITIONAL EXPERIMENTAL RESULTS
Complementary to experimental setups in Section VI, we
consider pure complex Gaussian channel noises. First, we
suppose that the transmit antennas emit continuous-valued
complex signals; without loss of generality, Gaussian signals
are used in experiments. The performance evaluation measure
is therefore the mean-squared error (MSE). The experimental
results are shown in Fig. 2.
20
40
60
80
Pilot Size
0
0.2
0.4
0.6
0.8
1
MSE
Capon
Kernel
Wiener
Wiener-CE
ZF
(a) N=8, SNR 10dB, Rv Estimated
20
40
60
80
Pilot Size
0
0.2
0.4
0.6
0.8
1
MSE
Capon
Kernel
Wiener
Wiener-CE
ZF
(b) N=8, SNR 10dB, Rv Known
20
40
60
80
Pilot Size
0
0.2
0.4
0.6
0.8
1
MSE
Capon
Kernel
Wiener
Wiener-CE
ZF
(c) N=16, SNR 10dB, Rv Estimated
20
40
60
80
Pilot Size
0
0.2
0.4
0.6
0.8
1
MSE
Capon
Kernel
Wiener
Wiener-CE
ZF
(d) N=16, SNR -10dB, Rv Esti-
mated
Fig. 2.
Testing MSE against training pilot sizes under different numbers
of receive antennas; only non-robust beamformers including non-diagonal-
loading ones are considered. The true value of Rv can be unknown and
estimated using pilot data. The signal-to-noise ratio (SNR) is 10dB or −10dB.
From Fig. 2, the following main points can be outlined.
1) For a fixed number M of transmit antennas, the larger
the number N of receive antennas, the smaller the MSE;
cf. Figs. 2(a) and 2(c). This fact is well-established and
is due to the benefit of antenna diversity. In addition, for
fixed N and M, the higher the SNR, the smaller the MSE;
cf. Figs. 2(c) and 2(d); this is also well believed.
2) As the pilot size increases, the Wiener beamformer
tends to have the best performance because the Wiener
beamformer is optimal for the linear Gaussian signal
model. When Rv is accurately known, the Wiener-CE
beamformer outperforms the general Wiener beamformer
(cf. Fig. 2(b)) because the former also exploits the in-
formation of the linear signal model in addition to the
pilot data, while the latter only utilizes the pilot data.
However, when Rv is estimated using the pilot data, the
performances of the general Wiener beamformer and the
Wiener-CE beamformer have no significant difference; cf.
Figs. 2(a) and 2(c). Therefore, Fig. 2 validates our claim
that channel estimation is not a necessary operation in
receive beamforming and estimation of wireless signals;
recall Subsection III-A3.
3) The ZF beamformer tends to be more efficient as N
increases; cf. Figs. 2(a) and 2(c). However, the ZF
beamformer becomes less satisfactory when the SNR
decreases; cf. Figs. 2(c) and 2(d). The Capon beamformer
is also unsatisfactory when N is small or the SNR is low.
4) The kernel beamformer, as a nonlinear method, cannot
outperform linear beamformers because, for a linear
Gaussian signal model, the optimal beamformer is linear.
From the perspective of machine learning, nonlinear
methods tend to overfit the limited training samples.
Second, we suppose that the transmit antennas emit discrete-
valued symbols from a constellation that is modulated using
quadrature phase-shift keying (QPSK). The performance eval-
uation measure is therefore the symbol error rate (SER). The
experimental results are shown in Fig. 3. We find that all
the conclusive main points from Fig. 2 can be obtained from
Fig. 3 as well: this validates that minimizing MSE reduces
SER. In addition, Figs. 3(c) and 3(d) reveal that the Wiener
beamformer even slightly works better than the Wiener-CE
beamformer when the pilot size is smaller than 15 because the
uncertainty in the estimated ˆRv, on the contrary, misleads the
latter. Nevertheless, as the pilot size increases, the Wiener-CE
beamformer tends to overlap the Wiener beamformer quickly.
20
40
60
80
Pilot Size
0
0.2
0.4
0.6
0.8
SER
Capon
Kernel
Wiener
Wiener-CE
ZF
(a) N=8, SNR 10dB, Rv Estimated
20
40
60
80
Pilot Size
0
0.2
0.4
0.6
0.8
SER
Capon
Kernel
Wiener
Wiener-CE
ZF
(b) N=8, SNR 10dB, Rv Known
20
40
60
80
Pilot Size
0
0.2
0.4
0.6
0.8
SER
Capon
Kernel
Wiener
Wiener-CE
ZF
(c) N=16, SNR 10dB, Rv Estimated
20
40
60
80
Pilot Size
0
0.2
0.4
0.6
0.8
SER
Capon
Kernel
Wiener
Wiener-CE
ZF
(d) N=16, SNR -10dB, Rv Esti-
mated
Fig. 3.
Testing SER against training pilot sizes under different numbers
of receive antennas; only non-robust beamformers including non-diagonal-
loading ones are considered. The true value of Rv can be unknown and
estimated using pilot data. The signal-to-noise ratio (SNR) is 10dB or −10dB.
